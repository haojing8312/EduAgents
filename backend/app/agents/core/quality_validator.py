"""
å¢å¼ºç‰ˆè´¨é‡éªŒè¯ç³»ç»Ÿ
ä¸“é—¨éªŒè¯è¯¾ç¨‹ç”Ÿæˆç»“æœæ˜¯å¦æ»¡è¶³åŸå§‹éœ€æ±‚
"""

import json
import logging
from datetime import datetime
from typing import Dict, List, Any, Tuple, Optional
from dataclasses import dataclass
from enum import Enum

logger = logging.getLogger(__name__)

class ValidationLevel(Enum):
    """éªŒè¯çº§åˆ«"""
    CRITICAL = "å…³é”®"     # å¿…é¡»é€šè¿‡
    IMPORTANT = "é‡è¦"   # å»ºè®®é€šè¿‡
    OPTIONAL = "å¯é€‰"    # ä¼˜åŒ–å»ºè®®

@dataclass
class ValidationResult:
    """éªŒè¯ç»“æœ"""
    check_name: str
    level: ValidationLevel
    passed: bool
    score: float  # 0-1åˆ†æ•°
    message: str
    suggestions: List[str]
    details: Dict[str, Any]

@dataclass
class QualityReport:
    """è´¨é‡æŠ¥å‘Š"""
    overall_score: float
    overall_passed: bool
    validation_results: List[ValidationResult]
    critical_issues: List[str]
    improvement_suggestions: List[str]
    validated_at: datetime
    metadata: Dict[str, Any]

class CourseQualityValidator:
    """è¯¾ç¨‹è´¨é‡éªŒè¯å™¨"""

    def __init__(self):
        self.validation_rules = self._initialize_validation_rules()

    def _initialize_validation_rules(self) -> Dict[str, Dict[str, Any]]:
        """åˆå§‹åŒ–éªŒè¯è§„åˆ™"""
        return {
            # éœ€æ±‚åŒ¹é…åº¦éªŒè¯
            "requirement_match": {
                "level": ValidationLevel.CRITICAL,
                "weight": 0.25,
                "description": "éªŒè¯è¯¾ç¨‹æ˜¯å¦æ»¡è¶³åŸå§‹éœ€æ±‚"
            },
            # å¹´é¾„é€‚å®œæ€§éªŒè¯
            "age_appropriateness": {
                "level": ValidationLevel.CRITICAL,
                "weight": 0.2,
                "description": "éªŒè¯å†…å®¹æ˜¯å¦é€‚åˆç›®æ ‡å¹´é¾„æ®µ"
            },
            # æ—¶é—´è§„åˆ’åˆç†æ€§
            "time_allocation": {
                "level": ValidationLevel.CRITICAL,
                "weight": 0.15,
                "description": "éªŒè¯æ—¶é—´å®‰æ’æ˜¯å¦åˆç†å¯è¡Œ"
            },
            # å†…å®¹ä¸“ä¸šæ€§
            "content_quality": {
                "level": ValidationLevel.IMPORTANT,
                "weight": 0.15,
                "description": "éªŒè¯å†…å®¹çš„ä¸“ä¸šæ€§å’Œæ·±åº¦"
            },
            # AIå·¥å…·é›†æˆ
            "ai_integration": {
                "level": ValidationLevel.IMPORTANT,
                "weight": 0.1,
                "description": "éªŒè¯AIå·¥å…·é›†æˆçš„åˆç†æ€§"
            },
            # PBLæ–¹æ³•è®º
            "pbl_methodology": {
                "level": ValidationLevel.IMPORTANT,
                "weight": 0.1,
                "description": "éªŒè¯PBLæ–¹æ³•è®ºçš„åº”ç”¨"
            },
            # å¯å®æ–½æ€§
            "feasibility": {
                "level": ValidationLevel.OPTIONAL,
                "weight": 0.05,
                "description": "éªŒè¯è¯¾ç¨‹å®æ–½çš„å¯è¡Œæ€§"
            }
        }

    def validate_course(
        self,
        generated_course: Dict[str, Any],
        original_requirements: Dict[str, Any],
        parsed_requirements: Optional[Dict[str, Any]] = None
    ) -> QualityReport:
        """éªŒè¯è¯¾ç¨‹è´¨é‡"""

        logger.info("ğŸ” å¼€å§‹è¯¾ç¨‹è´¨é‡éªŒè¯...")

        validation_results = []
        total_weight = 0
        weighted_score = 0

        for rule_name, rule_config in self.validation_rules.items():
            try:
                result = self._run_validation_rule(
                    rule_name, rule_config, generated_course,
                    original_requirements, parsed_requirements
                )
                validation_results.append(result)

                # è®¡ç®—åŠ æƒåˆ†æ•°
                weight = rule_config["weight"]
                total_weight += weight
                weighted_score += result.score * weight

                logger.info(f"âœ“ {rule_name}: {result.score:.2f} ({'é€šè¿‡' if result.passed else 'æœªé€šè¿‡'})")

            except Exception as e:
                logger.error(f"âŒ éªŒè¯è§„åˆ™ {rule_name} æ‰§è¡Œå¤±è´¥: {e}")
                # æ·»åŠ å¤±è´¥ç»“æœ
                validation_results.append(ValidationResult(
                    check_name=rule_name,
                    level=rule_config["level"],
                    passed=False,
                    score=0.0,
                    message=f"éªŒè¯æ‰§è¡Œå¤±è´¥: {str(e)}",
                    suggestions=["éœ€è¦æŠ€æœ¯å›¢é˜Ÿæ£€æŸ¥éªŒè¯é€»è¾‘"],
                    details={"error": str(e)}
                ))

        # è®¡ç®—æ€»åˆ†
        overall_score = weighted_score / total_weight if total_weight > 0 else 0

        # ç¡®å®šæ˜¯å¦é€šè¿‡ï¼ˆå…³é”®é¡¹ç›®å¿…é¡»å…¨éƒ¨é€šè¿‡ï¼‰
        critical_results = [r for r in validation_results if r.level == ValidationLevel.CRITICAL]
        overall_passed = all(r.passed for r in critical_results) and overall_score >= 0.7

        # æ”¶é›†é—®é¢˜å’Œå»ºè®®
        critical_issues = []
        improvement_suggestions = []

        for result in validation_results:
            if not result.passed and result.level == ValidationLevel.CRITICAL:
                critical_issues.append(f"{result.check_name}: {result.message}")
            if result.suggestions:
                improvement_suggestions.extend(result.suggestions)

        report = QualityReport(
            overall_score=overall_score,
            overall_passed=overall_passed,
            validation_results=validation_results,
            critical_issues=critical_issues,
            improvement_suggestions=improvement_suggestions,
            validated_at=datetime.now(),
            metadata={
                "validation_rules_count": len(self.validation_rules),
                "critical_rules_count": len(critical_results),
                "passed_rules_count": len([r for r in validation_results if r.passed])
            }
        )

        logger.info(f"âœ… è´¨é‡éªŒè¯å®Œæˆ - æ€»åˆ†: {overall_score:.0%}, {'é€šè¿‡' if overall_passed else 'æœªé€šè¿‡'}")
        return report

    def _run_validation_rule(
        self,
        rule_name: str,
        rule_config: Dict[str, Any],
        generated_course: Dict[str, Any],
        original_requirements: Dict[str, Any],
        parsed_requirements: Optional[Dict[str, Any]] = None
    ) -> ValidationResult:
        """æ‰§è¡Œå•ä¸ªéªŒè¯è§„åˆ™"""

        if rule_name == "requirement_match":
            return self._validate_requirement_match(
                generated_course, original_requirements, parsed_requirements
            )
        elif rule_name == "age_appropriateness":
            return self._validate_age_appropriateness(
                generated_course, original_requirements, parsed_requirements
            )
        elif rule_name == "time_allocation":
            return self._validate_time_allocation(
                generated_course, original_requirements, parsed_requirements
            )
        elif rule_name == "content_quality":
            return self._validate_content_quality(generated_course)
        elif rule_name == "ai_integration":
            return self._validate_ai_integration(generated_course, parsed_requirements)
        elif rule_name == "pbl_methodology":
            return self._validate_pbl_methodology(generated_course)
        elif rule_name == "feasibility":
            return self._validate_feasibility(generated_course)
        else:
            return ValidationResult(
                check_name=rule_name,
                level=rule_config["level"],
                passed=False,
                score=0.0,
                message="æœªçŸ¥çš„éªŒè¯è§„åˆ™",
                suggestions=[],
                details={}
            )

    def _validate_requirement_match(
        self, course: Dict[str, Any], original_req: Dict[str, Any], parsed_req: Optional[Dict[str, Any]]
    ) -> ValidationResult:
        """éªŒè¯éœ€æ±‚åŒ¹é…åº¦"""

        score = 0.0
        issues = []
        suggestions = []

        # ä½¿ç”¨è§£æåçš„éœ€æ±‚è¿›è¡Œç²¾ç¡®éªŒè¯
        if parsed_req:
            # éªŒè¯ä¸»é¢˜åŒ¹é…
            expected_topic = parsed_req.get('topic', '').lower()
            course_title = course.get('title', '').lower()

            if expected_topic and expected_topic in course_title:
                score += 0.3
            else:
                issues.append(f"è¯¾ç¨‹æ ‡é¢˜'{course_title}'ä¸é¢„æœŸä¸»é¢˜'{expected_topic}'ä¸åŒ¹é…")
                suggestions.append("è°ƒæ•´è¯¾ç¨‹æ ‡é¢˜ä»¥åæ˜ å…·ä½“ä¸»é¢˜")

            # éªŒè¯äº¤ä»˜ç‰©åŒ¹é…
            expected_deliverables = set(parsed_req.get('final_deliverables', []))
            course_deliverables = set(course.get('final_products', []))

            if expected_deliverables:
                match_ratio = len(expected_deliverables & course_deliverables) / len(expected_deliverables)
                score += 0.4 * match_ratio

                if match_ratio < 0.5:
                    issues.append(f"äº¤ä»˜ç‰©åŒ¹é…åº¦è¿‡ä½: æœŸæœ›{list(expected_deliverables)}, å®é™…{list(course_deliverables)}")
                    suggestions.append("è°ƒæ•´è¯¾ç¨‹è®¾è®¡ä»¥äº§å‡ºè¦æ±‚çš„å…·ä½“äº¤ä»˜ç‰©")

            # éªŒè¯å­¦ä¹ ç›®æ ‡åŒ¹é…
            expected_objectives = parsed_req.get('learning_objectives', [])
            course_objectives = course.get('learning_objectives', [])

            if expected_objectives and course_objectives:
                # ç®€å•çš„å…³é”®è¯åŒ¹é…æ£€æŸ¥
                obj_match_count = 0
                for expected_obj in expected_objectives:
                    for course_obj in course_objectives:
                        if any(word in course_obj.lower() for word in expected_obj.lower().split() if len(word) > 2):
                            obj_match_count += 1
                            break

                obj_match_ratio = obj_match_count / len(expected_objectives)
                score += 0.3 * obj_match_ratio

                if obj_match_ratio < 0.6:
                    issues.append("å­¦ä¹ ç›®æ ‡åŒ¹é…åº¦ä¸è¶³")
                    suggestions.append("é‡æ–°è°ƒæ•´å­¦ä¹ ç›®æ ‡ä»¥æ›´å¥½åŒ¹é…ç”¨æˆ·éœ€æ±‚")

        else:
            # å…œåº•éªŒè¯
            score = 0.5  # ç»™äºˆä¸­ç­‰åˆ†æ•°
            suggestions.append("å»ºè®®ä½¿ç”¨éœ€æ±‚è§£æç³»ç»Ÿæé«˜éªŒè¯ç²¾åº¦")

        passed = score >= 0.7
        message = "éœ€æ±‚åŒ¹é…åº¦è‰¯å¥½" if passed else f"éœ€æ±‚åŒ¹é…å­˜åœ¨é—®é¢˜: {'; '.join(issues)}"

        return ValidationResult(
            check_name="requirement_match",
            level=ValidationLevel.CRITICAL,
            passed=passed,
            score=score,
            message=message,
            suggestions=suggestions,
            details={"issues": issues, "parsed_req_available": parsed_req is not None}
        )

    def _validate_age_appropriateness(
        self, course: Dict[str, Any], original_req: Dict[str, Any], parsed_req: Optional[Dict[str, Any]]
    ) -> ValidationResult:
        """éªŒè¯å¹´é¾„é€‚å®œæ€§"""

        score = 0.0
        issues = []
        suggestions = []

        if parsed_req:
            expected_age_range = parsed_req.get('age_range', {})
            expected_min = expected_age_range.get('min', 0)
            expected_max = expected_age_range.get('max', 100)

            # æ£€æŸ¥è¯¾ç¨‹ä¸­çš„å¹´é¾„è®¾ç½®
            course_grades = course.get('grade_levels', [])
            if course_grades:
                # ç®€å•æ˜ å°„ï¼šå¹´çº§åˆ°å¹´é¾„
                grade_to_age = {
                    1: 6, 2: 7, 3: 8, 4: 9, 5: 10, 6: 11,  # å°å­¦
                    7: 12, 8: 13, 9: 14,  # åˆä¸­
                    10: 15, 11: 16, 12: 17  # é«˜ä¸­
                }

                course_ages = [grade_to_age.get(grade, grade) for grade in course_grades]
                course_min_age = min(course_ages)
                course_max_age = max(course_ages)

                # æ£€æŸ¥å¹´é¾„èŒƒå›´åŒ¹é…
                if expected_min <= course_min_age <= expected_max and expected_min <= course_max_age <= expected_max:
                    score += 0.8
                else:
                    issues.append(f"å¹´é¾„èŒƒå›´ä¸åŒ¹é…: æœŸæœ›{expected_min}-{expected_max}å², è¯¾ç¨‹{course_min_age}-{course_max_age}å²")
                    suggestions.append("è°ƒæ•´è¯¾ç¨‹éš¾åº¦å’Œå†…å®¹ä»¥é€‚é…æ­£ç¡®å¹´é¾„æ®µ")

            # æ£€æŸ¥è¯¾ç¨‹æŒç»­æ—¶é—´çš„å¹´é¾„é€‚å®œæ€§
            duration_weeks = course.get('duration_weeks', 0)
            if expected_min <= 12:  # å„¿ç«¥å’Œé’å°‘å¹´
                if duration_weeks > 8:
                    issues.append("è¯¾ç¨‹å‘¨æœŸè¿‡é•¿ï¼Œå¯èƒ½è¶…å‡ºä½å¹´é¾„æ®µæ³¨æ„åŠ›æŒç»­æ—¶é—´")
                    suggestions.append("è€ƒè™‘ç¼©çŸ­è¯¾ç¨‹å‘¨æœŸæˆ–åˆ†æˆå¤šä¸ªé˜¶æ®µ")
                else:
                    score += 0.2

        else:
            score = 0.5  # æ— æ³•éªŒè¯æ—¶ç»™äºˆä¸­ç­‰åˆ†æ•°

        passed = score >= 0.7
        message = "å¹´é¾„é€‚å®œæ€§è‰¯å¥½" if passed else f"å¹´é¾„é€‚å®œæ€§å­˜åœ¨é—®é¢˜: {'; '.join(issues)}"

        return ValidationResult(
            check_name="age_appropriateness",
            level=ValidationLevel.CRITICAL,
            passed=passed,
            score=score,
            message=message,
            suggestions=suggestions,
            details={"issues": issues}
        )

    def _validate_time_allocation(
        self, course: Dict[str, Any], original_req: Dict[str, Any], parsed_req: Optional[Dict[str, Any]]
    ) -> ValidationResult:
        """éªŒè¯æ—¶é—´åˆ†é…"""

        score = 0.0
        issues = []
        suggestions = []

        if parsed_req:
            expected_total_hours = parsed_req.get('total_duration', {}).get('total_hours', 0)
            course_hours = course.get('duration_hours', 0)

            if expected_total_hours > 0:
                # å…è®¸Â±20%çš„è¯¯å·®
                deviation = abs(course_hours - expected_total_hours) / expected_total_hours
                if deviation <= 0.2:
                    score += 0.6
                else:
                    issues.append(f"æ€»æ—¶é•¿åå·®è¿‡å¤§: æœŸæœ›{expected_total_hours}å°æ—¶, å®é™…{course_hours}å°æ—¶")
                    suggestions.append("è°ƒæ•´æ—¶é—´åˆ†é…ä»¥åŒ¹é…è¦æ±‚çš„æ€»æ—¶é•¿")

            # æ£€æŸ¥æ—¶é—´æ¨¡å¼åŒ¹é…
            expected_mode = parsed_req.get('time_mode', '')
            if 'é›†è®­è¥' in expected_mode:
                expected_days = parsed_req.get('total_duration', {}).get('days', 0)
                if expected_days > 0:
                    # é›†è®­è¥åº”è¯¥æ˜¯å¯†é›†å‹å®‰æ’
                    if course_hours / expected_days >= 4:  # æ¯å¤©è‡³å°‘4å°æ—¶
                        score += 0.4
                    else:
                        issues.append("é›†è®­è¥æ¨¡å¼çš„æ¯æ—¥æ—¶é•¿ä¸å¤Ÿå¯†é›†")
                        suggestions.append("å¢åŠ æ¯æ—¥å­¦ä¹ æ—¶é•¿ä»¥ç¬¦åˆé›†è®­è¥ç‰¹ç‚¹")

        else:
            score = 0.5

        passed = score >= 0.7
        message = "æ—¶é—´åˆ†é…åˆç†" if passed else f"æ—¶é—´åˆ†é…å­˜åœ¨é—®é¢˜: {'; '.join(issues)}"

        return ValidationResult(
            check_name="time_allocation",
            level=ValidationLevel.CRITICAL,
            passed=passed,
            score=score,
            message=message,
            suggestions=suggestions,
            details={"issues": issues}
        )

    def _validate_content_quality(self, course: Dict[str, Any]) -> ValidationResult:
        """éªŒè¯å†…å®¹è´¨é‡"""

        score = 0.0
        issues = []
        suggestions = []

        # æ£€æŸ¥å­¦ä¹ èµ„æºè´¨é‡
        resources = course.get('resources', [])
        if resources:
            # æ£€æŸ¥èµ„æºæ˜¯å¦é‡å¤æˆ–é€šç”¨åŒ–
            resource_titles = [r.get('title', '') for r in resources]
            unique_titles = set(resource_titles)

            if len(unique_titles) < len(resource_titles) * 0.8:
                issues.append("å­¦ä¹ èµ„æºé‡å¤åº¦è¿‡é«˜ï¼Œç¼ºä¹å¤šæ ·æ€§")
                suggestions.append("æä¾›æ›´å¤šæ ·åŒ–çš„å­¦ä¹ èµ„æº")
            else:
                score += 0.3

            # æ£€æŸ¥èµ„æºæè¿°è´¨é‡
            generic_descriptions = ['æ”¯æŒå­¦ä¹ çš„é‡è¦èµ„æº', 'å­¦ä¹ ææ–™', 'Introduction to Basic']
            generic_count = sum(1 for r in resources if any(g in r.get('description', '') for g in generic_descriptions))

            if generic_count > len(resources) * 0.5:
                issues.append("èµ„æºæè¿°è¿‡äºé€šç”¨åŒ–ï¼Œç¼ºä¹é’ˆå¯¹æ€§")
                suggestions.append("ä¸ºæ¯ä¸ªèµ„æºæä¾›å…·ä½“ã€ç›¸å…³çš„æè¿°")
            else:
                score += 0.3

        # æ£€æŸ¥å­¦ä¹ ç›®æ ‡è´¨é‡
        objectives = course.get('learning_objectives', [])
        if objectives:
            # æ£€æŸ¥ç›®æ ‡æ˜¯å¦å…·ä½“å¯è¡¡é‡
            vague_objectives = sum(1 for obj in objectives if any(word in obj.lower() for word in ['äº†è§£', 'çŸ¥é“', 'ç†è§£']) and 'å¦‚ä½•' not in obj.lower())

            if vague_objectives <= len(objectives) * 0.3:
                score += 0.2
            else:
                issues.append("å­¦ä¹ ç›®æ ‡è¿‡äºæ¨¡ç³Šï¼Œç¼ºä¹å¯æ“ä½œæ€§")
                suggestions.append("ä½¿ç”¨æ›´å…·ä½“ã€å¯è¡¡é‡çš„åŠ¨è¯æè¿°å­¦ä¹ ç›®æ ‡")

        # æ£€æŸ¥è¯¾ç¨‹é˜¶æ®µè®¾è®¡
        phases = course.get('phases', [])
        if phases:
            if len(phases) >= 3:  # è‡³å°‘æœ‰å¼€å§‹ã€å‘å±•ã€ç»“æŸé˜¶æ®µ
                score += 0.2
            else:
                issues.append("è¯¾ç¨‹é˜¶æ®µè®¾è®¡ä¸å®Œæ•´")
                suggestions.append("è®¾è®¡å®Œæ•´çš„è¯¾ç¨‹é˜¶æ®µï¼ŒåŒ…æ‹¬å¯¼å…¥ã€å‘å±•ã€æ€»ç»“")

        passed = score >= 0.6
        message = "å†…å®¹è´¨é‡è‰¯å¥½" if passed else f"å†…å®¹è´¨é‡éœ€è¦æ”¹è¿›: {'; '.join(issues)}"

        return ValidationResult(
            check_name="content_quality",
            level=ValidationLevel.IMPORTANT,
            passed=passed,
            score=score,
            message=message,
            suggestions=suggestions,
            details={"issues": issues}
        )

    def _validate_ai_integration(self, course: Dict[str, Any], parsed_req: Optional[Dict[str, Any]]) -> ValidationResult:
        """éªŒè¯AIå·¥å…·é›†æˆ"""

        score = 0.8  # åŸºç¡€åˆ†æ•°ï¼Œå‡è®¾AIé›†æˆåŸºæœ¬åˆç†
        issues = []
        suggestions = []

        if parsed_req:
            expected_ai_tools = parsed_req.get('ai_tools', [])

            # æ£€æŸ¥è¯¾ç¨‹ä¸­æ˜¯å¦æåˆ°äº†AIå·¥å…·çš„ä½¿ç”¨
            course_content = json.dumps(course, ensure_ascii=False).lower()

            for tool in expected_ai_tools:
                if tool.lower() in course_content:
                    score += 0.05  # æ¯ä¸ªåŒ¹é…çš„å·¥å…·åŠ åˆ†
                else:
                    suggestions.append(f"å»ºè®®åœ¨è¯¾ç¨‹ä¸­æ˜ç¡®è¯´æ˜å¦‚ä½•ä½¿ç”¨{tool}")

        passed = score >= 0.7
        message = "AIå·¥å…·é›†æˆåˆç†" if passed else "AIå·¥å…·é›†æˆéœ€è¦åŠ å¼º"

        return ValidationResult(
            check_name="ai_integration",
            level=ValidationLevel.IMPORTANT,
            passed=passed,
            score=min(score, 1.0),  # ç¡®ä¿ä¸è¶…è¿‡1.0
            message=message,
            suggestions=suggestions,
            details={"issues": issues}
        )

    def _validate_pbl_methodology(self, course: Dict[str, Any]) -> ValidationResult:
        """éªŒè¯PBLæ–¹æ³•è®º"""

        score = 0.0
        issues = []
        suggestions = []

        # æ£€æŸ¥æ˜¯å¦æœ‰é©±åŠ¨é—®é¢˜
        driving_question = course.get('driving_question', '')
        if driving_question and driving_question not in ['å¦‚ä½•è¿ç”¨æ‰€å­¦çŸ¥è¯†è§£å†³çœŸå®ä¸–ç•Œçš„é—®é¢˜ï¼Ÿ']:
            score += 0.3
        else:
            issues.append("ç¼ºä¹å…·ä½“çš„é©±åŠ¨é—®é¢˜")
            suggestions.append("è®¾è®¡ä¸ä¸»é¢˜ç›¸å…³çš„å…·ä½“é©±åŠ¨é—®é¢˜")

        # æ£€æŸ¥æ˜¯å¦æœ‰é¡¹ç›®æˆæœ
        final_products = course.get('final_products', [])
        if final_products and len(final_products) > 1:
            score += 0.3
        else:
            issues.append("é¡¹ç›®æˆæœè®¾è®¡ä¸å……åˆ†")
            suggestions.append("è®¾è®¡å¤šæ ·åŒ–çš„é¡¹ç›®æˆæœå±•ç¤ºæ–¹å¼")

        # æ£€æŸ¥è¯„ä¼°æ–¹å¼
        assessments = course.get('assessments', [])
        formative_count = sum(1 for a in assessments if a.get('type') == 'formative')
        summative_count = sum(1 for a in assessments if a.get('type') == 'summative')

        if formative_count > 0 and summative_count > 0:
            score += 0.4
        else:
            issues.append("è¯„ä¼°æ–¹å¼ä¸å¤Ÿå¤šå…ƒåŒ–")
            suggestions.append("ç»“åˆè¿‡ç¨‹æ€§è¯„ä»·å’Œç»ˆç»“æ€§è¯„ä»·")

        passed = score >= 0.6
        message = "PBLæ–¹æ³•è®ºåº”ç”¨è‰¯å¥½" if passed else f"PBLæ–¹æ³•è®ºåº”ç”¨éœ€è¦æ”¹è¿›: {'; '.join(issues)}"

        return ValidationResult(
            check_name="pbl_methodology",
            level=ValidationLevel.IMPORTANT,
            passed=passed,
            score=score,
            message=message,
            suggestions=suggestions,
            details={"issues": issues}
        )

    def _validate_feasibility(self, course: Dict[str, Any]) -> ValidationResult:
        """éªŒè¯å¯å®æ–½æ€§"""

        score = 0.7  # åŸºç¡€å¯å®æ–½æ€§åˆ†æ•°
        suggestions = [
            "å»ºè®®åœ¨å®æ–½å‰è¿›è¡Œå°è§„æ¨¡è¯•ç‚¹",
            "å‡†å¤‡å¤‡é€‰æ–¹æ¡ˆåº”å¯¹æŠ€æœ¯é—®é¢˜",
            "ç¡®ä¿æ•™å¸ˆå…·å¤‡å¿…è¦çš„æŠ€èƒ½æ”¯æŒ"
        ]

        return ValidationResult(
            check_name="feasibility",
            level=ValidationLevel.OPTIONAL,
            passed=True,
            score=score,
            message="è¯¾ç¨‹å…·æœ‰è‰¯å¥½çš„å¯å®æ–½æ€§",
            suggestions=suggestions,
            details={}
        )

    def format_report(self, report: QualityReport) -> str:
        """æ ¼å¼åŒ–è´¨é‡æŠ¥å‘Š"""

        output = f"""
ğŸ” ã€è¯¾ç¨‹è´¨é‡éªŒè¯æŠ¥å‘Šã€‘

ğŸ“Š æ€»ä½“è¯„ä¼°:
â€¢ ç»¼åˆå¾—åˆ†: {report.overall_score:.0%}
â€¢ éªŒè¯ç»“æœ: {'âœ… é€šè¿‡' if report.overall_passed else 'âŒ æœªé€šè¿‡'}
â€¢ éªŒè¯æ—¶é—´: {report.validated_at.strftime('%Y-%m-%d %H:%M:%S')}

ğŸ“‹ è¯¦ç»†éªŒè¯ç»“æœ:
"""

        for result in report.validation_results:
            status = "âœ…" if result.passed else "âŒ"
            output += f"""
{status} {result.check_name} ({result.level.value}) - {result.score:.0%}
   {result.message}
"""
            if result.suggestions:
                for suggestion in result.suggestions[:2]:  # æœ€å¤šæ˜¾ç¤º2ä¸ªå»ºè®®
                    output += f"   ğŸ’¡ {suggestion}\n"

        if report.critical_issues:
            output += "\nğŸš¨ å…³é”®é—®é¢˜:\n"
            for issue in report.critical_issues:
                output += f"â€¢ {issue}\n"

        if report.improvement_suggestions:
            output += "\nğŸ’¡ æ”¹è¿›å»ºè®®:\n"
            for suggestion in report.improvement_suggestions[:5]:  # æœ€å¤šæ˜¾ç¤º5ä¸ªå»ºè®®
                output += f"â€¢ {suggestion}\n"

        return output