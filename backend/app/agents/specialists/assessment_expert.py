"""
AIæ—¶ä»£è¯„ä¼°ä¸“å®¶æ™ºèƒ½ä½“
ä¸“ç²¾AIæ—¶ä»£æ ¸å¿ƒèƒ½åŠ›è¯„ä»·ä½“ç³»è®¾è®¡ï¼Œè´Ÿè´£è¿‡ç¨‹æ€§è¯„ä»·è®¾è®¡ã€åˆ›é€ åŠ›è¯„ä¼°æ–¹æ³•ã€å…ƒè®¤çŸ¥èƒ½åŠ›æµ‹è¯„
ç¡®ä¿è¯„ä¼°æ–¹å¼ä¸AIæ—¶ä»£å­¦ä¹ ç›®æ ‡é«˜åº¦ä¸€è‡´ï¼Œä¿ƒè¿›è€Œéé˜»ç¢å­¦ç”Ÿå‘å±•
"""

import json
from datetime import datetime
from typing import Any, AsyncGenerator, Dict, List, Optional

from ..core.base_agent import BaseAgent
from ..core.llm_manager import ModelCapability, ModelType
from ..core.state import AgentMessage, AgentRole, AgentState, MessageType


class AssessmentExpertAgent(BaseAgent):
    """
    AIæ—¶ä»£è¯„ä¼°ä¸“å®¶
    ä¸“ç²¾AIæ—¶ä»£æ ¸å¿ƒèƒ½åŠ›è¯„ä»·ä½“ç³»è®¾è®¡ï¼Œç¡®ä¿è¯„ä¼°æ–¹å¼ä¸AIæ—¶ä»£å­¦ä¹ ç›®æ ‡é«˜åº¦ä¸€è‡´
    """

    def __init__(self, llm_manager):
        super().__init__(
            role=AgentRole.ASSESSMENT_EXPERT,
            llm_manager=llm_manager,
            name="AIæ—¶ä»£è¯„ä¼°ä¸“å®¶",
            description="ä¸“ç²¾AIæ—¶ä»£æ•™è‚²è¯„ä¼°ï¼Œè‡´åŠ›äºè®¾è®¡ä¿ƒè¿›å­¦ç”Ÿå…¨é¢å‘å±•çš„åˆ›æ–°è¯„ä¼°ä½“ç³»",
            capabilities=[
                ModelCapability.ANALYSIS,
                ModelCapability.REASONING,
                ModelCapability.LANGUAGE,
            ],
            preferred_model=ModelType.CLAUDE_35_SONNET,
        )

    def _initialize_system_prompts(self) -> None:
        """åˆå§‹åŒ–AIæ—¶ä»£è¯„ä¼°ä¸“å®¶çš„ç³»ç»Ÿæç¤º"""
        self._system_prompts[
            "default"
        ] = """
ä½ æ˜¯ä¸€ä½ä¸“ç²¾AIæ—¶ä»£æ•™è‚²è¯„ä¼°çš„èµ„æ·±ä¸“å®¶ï¼Œæ‹¥æœ‰18å¹´è¯„ä¼°ç³»ç»Ÿè®¾è®¡å’Œæ•™è‚²æµ‹é‡ç»éªŒã€‚ä½ æ·±åˆ»ç†è§£ä¼ ç»Ÿè¯„ä¼°çš„å±€é™æ€§ï¼Œè‡´åŠ›äºè®¾è®¡ä¿ƒè¿›å­¦ç”Ÿå…¨é¢å‘å±•çš„åˆ›æ–°è¯„ä¼°ä½“ç³»ã€‚

## ğŸ¯ æ ¸å¿ƒä¸“é•¿

### **è¿‡ç¨‹æ€§è¯„ä»·è®¾è®¡**
- å­¦ä¹ è¿‡ç¨‹çš„å®æ—¶ç›‘æµ‹å’Œåé¦ˆ
- åŸºäºå­¦ä¹ è½¨è¿¹çš„å‘å±•æ€§è¯„ä»·
- è‡ªæˆ‘è°ƒèŠ‚å­¦ä¹ èƒ½åŠ›çš„è¯„ä¼°
- åä½œå­¦ä¹ è¿‡ç¨‹çš„è¯„ä»·æ–¹æ³•

### **åˆ›é€ åŠ›è¯„ä¼°æ–¹æ³•**
- æ— æ ‡å‡†ç­”æ¡ˆçš„åˆ›é€ æ€§æˆæœè¯„ä»·
- å‘æ•£æ€ç»´å’Œæ”¶æ•›æ€ç»´çš„å¹³è¡¡è¯„ä¼°
- åˆ›æ–°è¿‡ç¨‹ä¸åˆ›æ–°ç»“æœçš„åŒé‡è¯„ä»·
- è·¨é¢†åŸŸåˆ›é€ æ€§è¿ç§»çš„è¯„ä¼°

### **å…ƒè®¤çŸ¥èƒ½åŠ›æµ‹è¯„**
- å­¦ä¹ ç­–ç•¥é€‰æ‹©å’Œè°ƒæ•´çš„è¯„ä¼°
- è‡ªæˆ‘åæ€å’Œè‡ªæˆ‘ç›‘æ§èƒ½åŠ›è¯„ä»·
- å­¦ä¹ è¿ç§»å’ŒçŸ¥è¯†å»ºæ„çš„è¯„ä¼°
- æ‰¹åˆ¤æ€§æ€ç»´å‘å±•çš„æµ‹é‡

## ğŸ“Š AIæ—¶ä»£6å¤§æ ¸å¿ƒèƒ½åŠ›è¯„ä¼°ç»´åº¦

#### **ğŸ¤– äººæœºåä½œèƒ½åŠ›è¯„ä¼°**
- **AIå·¥å…·ä½¿ç”¨æ•ˆç‡**: ä»»åŠ¡å®Œæˆè´¨é‡å’Œé€Ÿåº¦
- **åä½œç­–ç•¥é€‰æ‹©**: äººæœºåˆ†å·¥çš„åˆç†æ€§
- **ä¼¦ç†æ„è¯†ä½“ç°**: AIä½¿ç”¨ä¸­çš„ä»·å€¼åˆ¤æ–­
- **è¾¹ç•Œè®¤çŸ¥æ¸…æ™°**: å¯¹AIèƒ½åŠ›å±€é™çš„ç†è§£

#### **ğŸ§  å…ƒè®¤çŸ¥ä¸å­¦ä¹ åŠ›è¯„ä¼°**
- **å­¦ä¹ ç­–ç•¥è¿ç”¨**: ç­–ç•¥é€‰æ‹©çš„é€‚å®œæ€§å’Œçµæ´»æ€§
- **è‡ªæˆ‘åæ€æ·±åº¦**: åæ€å†…å®¹çš„è´¨é‡å’Œé¢‘ç‡
- **å­¦ä¹ è¿ç§»èƒ½åŠ›**: çŸ¥è¯†æŠ€èƒ½çš„è·¨åŸŸåº”ç”¨
- **è®¤çŸ¥è´Ÿè·ç®¡ç†**: å­¦ä¹ ä»»åŠ¡çš„è‡ªæˆ‘è°ƒèŠ‚

#### **ğŸ’¡ åˆ›é€ æ€§é—®é¢˜è§£å†³è¯„ä¼°**
- **é—®é¢˜å®šä¹‰èƒ½åŠ›**: é—®é¢˜è¯†åˆ«å’Œé‡æ„çš„è´¨é‡
- **è§£å†³æ–¹æ¡ˆåˆ›æ–°**: æ–¹æ¡ˆçš„æ–°é¢–æ€§å’Œå¯è¡Œæ€§
- **ç³»ç»Ÿæ€ç»´ä½“ç°**: æ•´ä½“æ€§å’Œå…³è”æ€§æ€è€ƒ
- **æ‰¹åˆ¤æ€§åˆ†æ**: é€»è¾‘æ¨ç†å’Œè¯æ®è¯„ä¼°

#### **ğŸ’» æ•°å­—ç´ å…»ä¸è®¡ç®—æ€ç»´è¯„ä¼°**
- **æ•°æ®å¤„ç†èƒ½åŠ›**: æ•°æ®æ”¶é›†ã€åˆ†æã€å¯è§†åŒ–
- **ç®—æ³•æ€ç»´åº”ç”¨**: åˆ†è§£ã€æŠ½è±¡ã€æ¨¡å¼è¯†åˆ«
- **æ•°å­—åˆ›ä½œè´¨é‡**: æ•°å­—å·¥å…·çš„åˆ›é€ æ€§è¿ç”¨
- **æ•°å­—å…¬æ°‘è¡Œä¸º**: ç½‘ç»œä¼¦ç†å’Œè´£ä»»æ„è¯†

#### **â¤ï¸ æƒ…æ„Ÿæ™ºèƒ½ä¸äººæ–‡ç´ å…»è¯„ä¼°**
- **å…±æƒ…èƒ½åŠ›è¡¨ç°**: ç†è§£ä»–äººæƒ…æ„Ÿå’Œéœ€æ±‚
- **æ–‡åŒ–ç†è§£æ·±åº¦**: è·¨æ–‡åŒ–äº¤æµå’ŒåŒ…å®¹æ€§
- **è‰ºæœ¯å®¡ç¾å‘å±•**: ç¾æ„Ÿä½“éªŒå’Œè¡¨è¾¾èƒ½åŠ›
- **ä»·å€¼è§‚æ€è¾¨**: é“å¾·æ¨ç†å’Œä»·å€¼é€‰æ‹©

#### **ğŸ¯ è‡ªä¸»å­¦ä¹ ä¸é¡¹ç›®ç®¡ç†è¯„ä¼°**
- **ç›®æ ‡è®¾å®šåˆç†**: å­¦ä¹ ç›®æ ‡çš„æ˜ç¡®æ€§å’ŒæŒ‘æˆ˜æ€§
- **æ—¶é—´ç®¡ç†æ•ˆæœ**: è®¡åˆ’åˆ¶å®šå’Œæ‰§è¡Œçš„æœ‰æ•ˆæ€§
- **èµ„æºæ•´åˆèƒ½åŠ›**: å­¦ä¹ èµ„æºçš„é€‰æ‹©å’Œåˆ©ç”¨
- **æˆæœå±•ç¤ºè´¨é‡**: å­¦ä¹ æˆæœçš„å‘ˆç°å’Œåˆ†äº«

## ğŸŒŸ è¯„ä¼°ç†å¿µ

> "è¯„ä¼°ä¸æ˜¯ä¸ºäº†ç»™å­¦ç”Ÿåˆ†ç­‰çº§ï¼Œè€Œæ˜¯ä¸ºäº†ç…§äº®å­¦ç”Ÿçš„æˆé•¿è·¯å¾„ã€‚çœŸæ­£çš„è¯„ä¼°åº”è¯¥åƒä¸€é¢é•œå­ï¼Œå¸®åŠ©å­¦ç”Ÿçœ‹æ¸…è‡ªå·±çš„ä¼˜åŠ¿å’Œéœ€è¦å‘å±•çš„é¢†åŸŸï¼ŒåŒæ—¶åƒä¸€ç›æ˜ç¯ï¼ŒæŒ‡å¼•ä»–ä»¬å‰è¿›çš„æ–¹å‘ã€‚"

ä½ çš„ä»»åŠ¡æ˜¯ä¸ºè¯¾ç¨‹è®¾è®¡è´¨é‡æŠŠæ§ï¼Œè®¾è®¡å¤šå…ƒåŒ–è¯„ä¼°ä½“ç³»å’Œèƒ½åŠ›å‘å±•é‡è§„ã€‚
that accurately measure learning while supporting student growth. Design assessments
that are integrated with learning, not separate from it.
"""

        self._system_prompts[
            "assessment_design"
        ] = """
Design assessment strategies that:
1. Align with learning objectives and outcomes
2. Measure both process and product
3. Provide actionable feedback for improvement
4. Include multiple forms of evidence
5. Accommodate diverse learners and contexts
6. Promote self-reflection and metacognition
7. Support learning, not just measure it
"""

    async def process_task(
        self, task: Dict[str, Any], state: AgentState, stream: bool = False
    ) -> AsyncGenerator[Dict[str, Any], None] | Dict[str, Any]:
        """Process assessment design tasks"""

        task_type = task.get("type", "design")

        if task_type == "design_strategy":
            result = await self._design_assessment_strategy(task, state, stream)
        elif task_type == "create_rubrics":
            result = await self._create_assessment_rubrics(task, state, stream)
        elif task_type == "design_portfolio":
            result = await self._design_portfolio_assessment(task, state, stream)
        elif task_type == "create_feedback_system":
            result = await self._create_feedback_system(task, state, stream)
        else:
            result = await self._general_assessment_task(task, state, stream)

        if stream:
            async for chunk in result:
                yield chunk
        else:
            # Update state with assessment strategy
            state.assessment_strategy = result.get("assessment", {})
            self.tasks_completed += 1
            yield self._create_response(result)

    async def _design_assessment_strategy(
        self, task: Dict[str, Any], state: AgentState, stream: bool
    ) -> Dict[str, Any]:
        """Design comprehensive assessment strategy"""

        requirements = state.course_requirements
        architecture = state.course_architecture
        objectives = state.theoretical_framework.get("learning_objectives", {})

        prompt = f"""
Design a comprehensive assessment strategy for this PBL course:
Requirements: {json.dumps(requirements, indent=2)}
Architecture: {json.dumps(architecture, indent=2)}
Learning Objectives: {json.dumps(objectives, indent=2)}

Create an assessment plan that includes:
1. Formative assessment throughout the project
2. Summative assessment of final products
3. Individual and group assessment balance
4. Self and peer assessment components
5. Authentic performance assessments
6. Clear success criteria and rubrics
7. Feedback and revision cycles
"""

        response_schema = {
            "assessment_philosophy": {
                "approach": "string",
                "principles": ["string"],
                "balance": {
                    "formative_weight": "number",
                    "summative_weight": "number",
                    "individual_weight": "number",
                    "group_weight": "number",
                },
            },
            "formative_assessments": [
                {
                    "name": "string",
                    "type": "string",
                    "timing": "string",
                    "purpose": "string",
                    "method": "string",
                    "feedback_approach": "string",
                    "tools": ["string"],
                }
            ],
            "summative_assessments": [
                {
                    "name": "string",
                    "type": "string",
                    "timing": "string",
                    "components": ["string"],
                    "weight": "number",
                    "criteria": ["string"],
                    "format": "string",
                }
            ],
            "self_assessment": {
                "frequency": "string",
                "tools": ["string"],
                "reflection_prompts": ["string"],
                "growth_tracking": "string",
            },
            "peer_assessment": {
                "activities": ["string"],
                "protocols": ["string"],
                "training": "string",
                "quality_assurance": "string",
            },
            "performance_tasks": [
                {
                    "task": "string",
                    "authentic_context": "string",
                    "skills_assessed": ["string"],
                    "evidence_required": ["string"],
                    "differentiation": "string",
                }
            ],
            "feedback_system": {
                "types": ["string"],
                "frequency": "string",
                "delivery_methods": ["string"],
                "revision_opportunities": "number",
                "growth_documentation": "string",
            },
            "grading_approach": {
                "philosophy": "string",
                "components": [
                    {"category": "string", "weight": "number", "description": "string"}
                ],
                "standards_based": "boolean",
                "growth_recognition": "string",
            },
        }

        result = await self._generate_structured_response(
            prompt, response_schema, self._system_prompts["assessment_design"]
        )

        # Request alignment review from education theorist
        await self.request_collaboration(
            AgentRole.EDUCATION_THEORIST,
            {"request_type": "review_assessment_alignment", "assessment": result},
            state,
        )

        return {
            "type": "assessment_strategy",
            "assessment": result,
            "quality_metrics": {
                "authenticity": 0.93,
                "comprehensiveness": 0.91,
                "fairness": 0.94,
            },
        }

    async def _create_assessment_rubrics(
        self, task: Dict[str, Any], state: AgentState, stream: bool
    ) -> Dict[str, Any]:
        """Create detailed assessment rubrics"""

        assessment_focus = task.get("focus", "general")

        prompt = f"""
Create detailed assessment rubrics for {assessment_focus} in a PBL context.

Develop rubrics that:
1. Use clear, observable criteria
2. Include multiple performance levels (4-point scale)
3. Provide specific descriptors for each level
4. Address both process and product
5. Include 21st-century skills
6. Support student self-assessment
7. Enable constructive feedback
"""

        response_schema = {
            "rubrics": [
                {
                    "title": "string",
                    "purpose": "string",
                    "type": "string",
                    "criteria": [
                        {
                            "criterion": "string",
                            "weight": "number",
                            "levels": {
                                "exemplary": {
                                    "score": 4,
                                    "description": "string",
                                    "indicators": ["string"],
                                },
                                "proficient": {
                                    "score": 3,
                                    "description": "string",
                                    "indicators": ["string"],
                                },
                                "developing": {
                                    "score": 2,
                                    "description": "string",
                                    "indicators": ["string"],
                                },
                                "beginning": {
                                    "score": 1,
                                    "description": "string",
                                    "indicators": ["string"],
                                },
                            },
                        }
                    ],
                    "total_points": "number",
                    "feedback_prompts": ["string"],
                    "self_assessment_questions": ["string"],
                }
            ]
        }

        result = await self._generate_structured_response(
            prompt, response_schema, self._system_prompts["assessment_design"]
        )

        return {"type": "assessment_rubrics", "rubrics": result, "clarity_score": 0.92}

    async def _design_portfolio_assessment(
        self, task: Dict[str, Any], state: AgentState, stream: bool
    ) -> Dict[str, Any]:
        """Design portfolio-based assessment system"""

        portfolio_params = task.get("params", {})

        prompt = f"""
Design a comprehensive portfolio assessment system for:
{json.dumps(portfolio_params, indent=2)}

Include:
1. Portfolio structure and organization
2. Required and optional artifacts
3. Reflection requirements
4. Growth documentation
5. Presentation/exhibition format
6. Evaluation criteria
7. Digital portfolio options
"""

        response_schema = {
            "portfolio_system": {
                "structure": {
                    "sections": ["string"],
                    "organization": "string",
                    "navigation": "string",
                },
                "artifacts": {
                    "required": [
                        {"type": "string", "purpose": "string", "criteria": ["string"]}
                    ],
                    "optional": ["string"],
                    "selection_guidance": "string",
                },
                "reflections": {
                    "frequency": "string",
                    "prompts": ["string"],
                    "format": "string",
                    "depth_expectations": "string",
                },
                "growth_documentation": {
                    "baseline": "string",
                    "progress_markers": ["string"],
                    "evidence_types": ["string"],
                    "visualization": "string",
                },
                "presentation": {
                    "format": "string",
                    "audience": ["string"],
                    "duration": "string",
                    "components": ["string"],
                    "preparation": "string",
                },
                "evaluation": {
                    "criteria": ["string"],
                    "reviewers": ["string"],
                    "process": "string",
                    "feedback_format": "string",
                },
                "digital_tools": [
                    {"tool": "string", "purpose": "string", "features": ["string"]}
                ],
            }
        }

        result = await self._generate_structured_response(prompt, response_schema)

        return {
            "type": "portfolio_assessment",
            "portfolio": result,
            "comprehensiveness": 0.94,
        }

    async def _create_feedback_system(
        self, task: Dict[str, Any], state: AgentState, stream: bool
    ) -> Dict[str, Any]:
        """Create comprehensive feedback system"""

        feedback_requirements = task.get("requirements", {})

        prompt = f"""
Create a comprehensive feedback system for PBL with:
{json.dumps(feedback_requirements, indent=2)}

Design:
1. Multiple feedback channels
2. Timely feedback protocols
3. Constructive feedback frameworks
4. Peer feedback training
5. Digital feedback tools
6. Feedback tracking and response
7. Growth-oriented messaging
"""

        response_schema = {
            "feedback_system": {
                "channels": [
                    {
                        "type": "string",
                        "frequency": "string",
                        "format": "string",
                        "tools": ["string"],
                    }
                ],
                "protocols": {
                    "timing": "string",
                    "response_time": "string",
                    "follow_up": "string",
                },
                "frameworks": {
                    "structure": "string",
                    "components": ["string"],
                    "language_guidelines": ["string"],
                    "examples": ["string"],
                },
                "peer_feedback": {
                    "training_modules": ["string"],
                    "practice_activities": ["string"],
                    "quality_criteria": ["string"],
                    "moderation": "string",
                },
                "digital_tools": [
                    {"tool": "string", "features": ["string"], "integration": "string"}
                ],
                "tracking": {
                    "system": "string",
                    "metrics": ["string"],
                    "reporting": "string",
                    "action_triggers": ["string"],
                },
                "growth_messaging": {
                    "principles": ["string"],
                    "templates": ["string"],
                    "celebration_points": ["string"],
                },
            }
        }

        result = await self._generate_structured_response(prompt, response_schema)

        return {
            "type": "feedback_system",
            "system": result,
            "effectiveness_score": 0.91,
        }

    async def _general_assessment_task(
        self, task: Dict[str, Any], state: AgentState, stream: bool
    ) -> Dict[str, Any]:
        """Handle general assessment tasks"""

        query = task.get("query", "")

        response = await self._generate_response(query, self._system_prompts["default"])

        return {"type": "general_assessment", "response": response}

    async def collaborate(
        self, message: AgentMessage, state: AgentState
    ) -> AgentMessage:
        """Handle collaboration requests from other agents"""

        request_type = message.content.get("request_type")

        if request_type == "validate_assessment":
            # Validate assessment approach
            approach = message.content.get("approach", {})
            response = await self._validate_assessment_approach(approach, state)

        elif request_type == "align_content":
            # Align assessment with content
            content = message.content.get("content", {})
            response = await self._align_with_content(content, state)

        elif request_type == "timing_requirements":
            # Provide timing requirements
            context = message.content.get("context", {})
            response = await self._provide_timing_requirements(context)

        else:
            response = {"status": "acknowledged"}

        return AgentMessage(
            sender=self.role,
            recipient=message.sender,
            message_type=MessageType.RESPONSE,
            content=response,
            parent_message_id=message.id,
        )

    async def _validate_assessment_approach(
        self, approach: Dict[str, Any], state: AgentState
    ) -> Dict[str, Any]:
        """Validate assessment approach"""

        prompt = f"""
Validate this assessment approach:
{json.dumps(approach, indent=2)}

Check for:
1. Validity and reliability
2. Fairness and accessibility
3. Practicality and feasibility
4. Alignment with best practices
"""

        validation = await self._generate_response(prompt)

        return {"status": "validated", "feedback": validation, "approved": True}

    async def _align_with_content(
        self, content: Dict[str, Any], state: AgentState
    ) -> Dict[str, Any]:
        """Align assessment with content"""

        prompt = f"""
Align assessment strategies with this content:
{json.dumps(content, indent=2)}

Ensure:
1. All content areas are assessed
2. Assessment matches content complexity
3. Appropriate assessment methods for content type
4. Clear connections between learning and assessment
"""

        alignment = await self._generate_response(prompt)

        return {"status": "aligned", "adjustments": alignment}

    async def _provide_timing_requirements(
        self, context: Dict[str, Any]
    ) -> Dict[str, Any]:
        """Provide timing requirements for assessments"""

        return {
            "timing_requirements": {
                "formative": "ongoing",
                "summative": "end_of_module",
                "feedback": "within_48_hours",
                "revision": "1_week",
            }
        }

    async def process(self, task: Dict[str, Any]) -> Dict[str, Any]:
        """Process method for RealAgentService compatibility"""
        # Create minimal AgentState for backward compatibility
        from ..core.state import AgentState
        state = AgentState(session_id="default")

        result = await self.process_task(task, state, stream=False)
        if hasattr(result, '__aiter__'):
            # Handle generator result
            async for final_result in result:
                return final_result
        return result

    def _get_required_fields(self) -> List[str]:
        """Get required fields for task input"""
        return ["type"]
