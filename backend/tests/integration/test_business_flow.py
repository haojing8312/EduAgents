"""
å®Œæ•´ä¸šåŠ¡ç©¿è¶Šæµ‹è¯•è„šæœ¬
æ¨¡æ‹Ÿå‰ç«¯ä¸šåŠ¡æµç¨‹ï¼Œæµ‹è¯•æ ¸å¿ƒAPIæ¥å£çš„å®Œæ•´æ€§å’Œå¯ç”¨æ€§
"""

import asyncio
import json
import logging
import os
import sys
import time
from datetime import datetime
from pathlib import Path
from typing import Any, Dict, List, Optional
from uuid import UUID

import httpx
import pytest
from faker import Faker

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.insert(0, str(Path(__file__).parent.parent.parent))

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# åˆå§‹åŒ–faker
fake = Faker('zh_CN')


class BusinessFlowTester:
    """ä¸šåŠ¡æµç¨‹æµ‹è¯•å™¨ - å¢å¼ºç‰ˆï¼Œé›†æˆåä½œè¿½è¸ªåŠŸèƒ½éªŒè¯"""

    def __init__(self, base_url: str = "http://localhost:48284"):
        self.base_url = base_url
        # åˆ›å»ºhttpxå®¢æˆ·ç«¯æ—¶ç¦ç”¨ç¯å¢ƒå˜é‡ä»£ç†ï¼Œé¿å…æµ‹è¯•æ—¶çš„ç½‘ç»œé—®é¢˜
        self.client = httpx.AsyncClient(timeout=60.0, trust_env=False)
        self.session_data = {}
        self.test_results = {}
        self.collaboration_data = {}

        # åˆ›å»ºexportsç›®å½•ç»“æ„
        self.exports_dir = Path(__file__).parent.parent.parent / "exports" / "business_tests"
        self.test_start_time = datetime.now()
        self.test_run_dir = self.exports_dir / f"test_run_{self.test_start_time.strftime('%Y%m%d_%H%M%S')}"
        self.test_run_dir.mkdir(parents=True, exist_ok=True)

    async def cleanup(self):
        """æ¸…ç†èµ„æº"""
        await self.client.aclose()

    async def wait_and_check_health(self, retries: int = 30, delay: float = 2.0) -> bool:
        """ç­‰å¾…æœåŠ¡å¯åŠ¨å¹¶æ£€æŸ¥å¥åº·çŠ¶æ€"""
        logger.info("ç­‰å¾…åç«¯æœåŠ¡å¯åŠ¨...")

        for i in range(retries):
            try:
                response = await self.client.get(f"{self.base_url}/api/health")
                if response.status_code == 200:
                    health_data = response.json()
                    logger.info(f"æœåŠ¡å¥åº·æ£€æŸ¥é€šè¿‡: {health_data}")
                    return True
            except Exception as e:
                logger.info(f"ç­‰å¾…æœåŠ¡å¯åŠ¨ ({i+1}/{retries}): {str(e)}")

            if i < retries - 1:
                await asyncio.sleep(delay)

        logger.error("æœåŠ¡å¯åŠ¨è¶…æ—¶æˆ–å¥åº·æ£€æŸ¥å¤±è´¥")
        return False

    def log_test_result(self, test_name: str, success: bool, details: Optional[Dict] = None):
        """è®°å½•æµ‹è¯•ç»“æœ"""
        result = {
            "success": success,
            "timestamp": datetime.now().isoformat(),
            "details": details or {}
        }
        self.test_results[test_name] = result
        status = "âœ…" if success else "âŒ"
        logger.info(f"{status} {test_name}")

    async def export_collaboration_data(self, session_id: str):
        """å¯¼å‡ºåä½œè¿½è¸ªæ•°æ®"""
        try:
            logger.info("ğŸ” å¯¼å‡ºåä½œè¿½è¸ªæ•°æ®...")

            # å¯¼å‡ºåä½œæµç¨‹æ•°æ®
            try:
                response = await self.client.get(f"{self.base_url}/api/v1/collaboration/sessions/{session_id}/flow")
                if response.status_code == 200:
                    flow_data = response.json()
                    with open(self.test_run_dir / "collaboration_flow.json", 'w', encoding='utf-8') as f:
                        json.dump(flow_data, f, ensure_ascii=False, indent=2)
                    logger.info("âœ… åä½œæµç¨‹æ•°æ®å¯¼å‡ºæˆåŠŸ")
                    self.collaboration_data["flow"] = flow_data
                else:
                    logger.warning(f"âš ï¸ åä½œæµç¨‹æ•°æ®å¯¼å‡ºå¤±è´¥: HTTP {response.status_code}")
            except Exception as e:
                logger.warning(f"âš ï¸ åä½œæµç¨‹æ•°æ®å¯¼å‡ºå¼‚å¸¸: {e}")

            # å¯¼å‡ºAIè°ƒç”¨åˆ†ææ•°æ®
            try:
                response = await self.client.get(f"{self.base_url}/api/v1/collaboration/sessions/{session_id}/ai-calls")
                if response.status_code == 200:
                    ai_data = response.json()
                    with open(self.test_run_dir / "ai_calls_analytics.json", 'w', encoding='utf-8') as f:
                        json.dump(ai_data, f, ensure_ascii=False, indent=2)
                    logger.info("âœ… AIè°ƒç”¨åˆ†ææ•°æ®å¯¼å‡ºæˆåŠŸ")
                    self.collaboration_data["ai_calls"] = ai_data
                else:
                    logger.warning(f"âš ï¸ AIè°ƒç”¨åˆ†ææ•°æ®å¯¼å‡ºå¤±è´¥: HTTP {response.status_code}")
            except Exception as e:
                logger.warning(f"âš ï¸ AIè°ƒç”¨åˆ†ææ•°æ®å¯¼å‡ºå¼‚å¸¸: {e}")

            # å¯¼å‡ºäº¤ä»˜ç‰©è¿½è¸ªæ•°æ®
            try:
                response = await self.client.get(f"{self.base_url}/api/v1/collaboration/sessions/{session_id}/deliverables")
                if response.status_code == 200:
                    deliverable_data = response.json()
                    with open(self.test_run_dir / "deliverable_traceability.json", 'w', encoding='utf-8') as f:
                        json.dump(deliverable_data, f, ensure_ascii=False, indent=2)
                    logger.info("âœ… äº¤ä»˜ç‰©è¿½è¸ªæ•°æ®å¯¼å‡ºæˆåŠŸ")
                    self.collaboration_data["deliverables"] = deliverable_data
                else:
                    logger.warning(f"âš ï¸ äº¤ä»˜ç‰©è¿½è¸ªæ•°æ®å¯¼å‡ºå¤±è´¥: HTTP {response.status_code}")
            except Exception as e:
                logger.warning(f"âš ï¸ äº¤ä»˜ç‰©è¿½è¸ªæ•°æ®å¯¼å‡ºå¼‚å¸¸: {e}")

            # å¯¼å‡ºå®Œæ•´çš„åä½œæŠ¥å‘Š
            try:
                response = await self.client.get(f"{self.base_url}/api/v1/collaboration/sessions/{session_id}/export?format_type=json")
                if response.status_code == 200:
                    export_data = response.json()
                    with open(self.test_run_dir / "complete_collaboration_report.json", 'w', encoding='utf-8') as f:
                        json.dump(export_data, f, ensure_ascii=False, indent=2)
                    logger.info("âœ… å®Œæ•´åä½œæŠ¥å‘Šå¯¼å‡ºæˆåŠŸ")
                    self.collaboration_data["complete_report"] = export_data
                else:
                    logger.warning(f"âš ï¸ å®Œæ•´åä½œæŠ¥å‘Šå¯¼å‡ºå¤±è´¥: HTTP {response.status_code}")
            except Exception as e:
                logger.warning(f"âš ï¸ å®Œæ•´åä½œæŠ¥å‘Šå¯¼å‡ºå¼‚å¸¸: {e}")

            # è®°å½•åä½œè¿½è¸ªæ•°æ®å¯¼å‡ºç»“æœ
            exported_files = []
            for filename in ["collaboration_flow.json", "ai_calls_analytics.json", "deliverable_traceability.json", "complete_collaboration_report.json"]:
                filepath = self.test_run_dir / filename
                if filepath.exists():
                    exported_files.append(filename)

            self.log_test_result("åä½œæ•°æ®å¯¼å‡º", len(exported_files) > 0, {
                "exported_files": exported_files,
                "export_count": len(exported_files)
            })

        except Exception as e:
            logger.error(f"âŒ åä½œæ•°æ®å¯¼å‡ºå¤±è´¥: {e}")
            self.log_test_result("åä½œæ•°æ®å¯¼å‡º", False, {"error": str(e)})

    async def _handle_failed_session(self, session_id: str, error_msg: str):
        """å¤„ç†å¤±è´¥çš„ä¼šè¯ï¼Œè®°å½•å¤±è´¥ä¿¡æ¯å¹¶å°è¯•å¯¼å‡ºåä½œè¿½è¸ªæ•°æ®ï¼ˆç”¨äºé—®é¢˜åˆ†æï¼‰"""
        try:
            # è®°å½•å¤±è´¥ä¿¡æ¯
            failure_info = {
                "status": "failed",
                "error": error_msg,
                "session_id": session_id,
                "failure_timestamp": datetime.now().isoformat(),
                "note": "æ­¤ä¼šè¯å·²å¤±è´¥ï¼Œæ— æ³•æä¾›è¯¾ç¨‹è®¾è®¡ç»“æœã€‚ä»¥ä¸‹åä½œæ•°æ®ä»…ç”¨äºé—®é¢˜åˆ†æã€‚"
            }

            # ä¿å­˜å¤±è´¥è®°å½•ï¼ˆæ˜ç¡®æ ‡æ³¨ä¸ºå¤±è´¥ï¼Œä¸åŒ…å«ä»»ä½•å¯èƒ½è¯¯å¯¼çš„ç»“æœï¼‰
            with open(self.test_run_dir / "session_failure_record.json", 'w', encoding='utf-8') as f:
                json.dump(failure_info, f, ensure_ascii=False, indent=2)
            logger.info("ğŸ“ å¤±è´¥è®°å½•å·²ä¿å­˜")

            # å°è¯•å¯¼å‡ºåä½œè¿½è¸ªæ•°æ®ï¼ˆä»…ç”¨äºè°ƒè¯•å’Œé—®é¢˜åˆ†æï¼‰
            logger.info("ğŸ” å°è¯•å¯¼å‡ºåä½œè¿½è¸ªæ•°æ®ç”¨äºé—®é¢˜åˆ†æ...")
            await self.export_collaboration_data(session_id)

        except Exception as e:
            logger.warning(f"âš ï¸ å¤„ç†å¤±è´¥ä¼šè¯æ—¶å‡ºç°å¼‚å¸¸: {e}")

    def _validate_course_design_result(self, result_data: Dict[str, Any]) -> Dict[str, Any]:
        """éªŒè¯è¯¾ç¨‹è®¾è®¡ç»“æœçš„è´¨é‡"""
        quality_check = {
            "has_overview": bool(result_data.get("course_overview")),
            "has_content": bool(result_data.get("content")),
            "has_assessment": bool(result_data.get("assessment")),
            "has_materials": bool(result_data.get("materials")),
            "overall_quality": "poor"
        }

        # æ£€æŸ¥è¯¾ç¨‹æ¦‚è§ˆè´¨é‡
        overview = result_data.get("course_overview", {})
        if overview:
            quality_check["overview_details"] = {
                "has_requirements": bool(overview.get("requirements")),
                "has_theoretical_foundation": bool(overview.get("theoretical_foundation")),
                "has_architecture": bool(overview.get("architecture"))
            }

        # æ£€æŸ¥å†…å®¹è´¨é‡
        content = result_data.get("content", {})
        if content:
            modules = content.get("modules", [])
            quality_check["content_details"] = {
                "module_count": len(modules),
                "has_modules": len(modules) > 0
            }

        # ç»¼åˆè´¨é‡è¯„ä¼°
        quality_score = sum([
            quality_check["has_overview"],
            quality_check["has_content"],
            quality_check["has_assessment"],
            quality_check["has_materials"]
        ])

        if quality_score >= 4:
            quality_check["overall_quality"] = "excellent"
        elif quality_score >= 3:
            quality_check["overall_quality"] = "good"
        elif quality_score >= 2:
            quality_check["overall_quality"] = "fair"

        return quality_check

    async def test_root_endpoint(self) -> bool:
        """æµ‹è¯•æ ¹ç«¯ç‚¹"""
        try:
            response = await self.client.get(f"{self.base_url}/")
            success = response.status_code == 200 and "PBLæ™ºèƒ½åŠ©æ‰‹ API" in response.json().get("service", "")
            self.log_test_result("æ ¹ç«¯ç‚¹æµ‹è¯•", success, {"status_code": response.status_code})
            return success
        except Exception as e:
            self.log_test_result("æ ¹ç«¯ç‚¹æµ‹è¯•", False, {"error": str(e)})
            return False

    async def test_system_capabilities(self) -> bool:
        """æµ‹è¯•ç³»ç»Ÿèƒ½åŠ›æŸ¥è¯¢"""
        try:
            response = await self.client.get(f"{self.base_url}/api/v1/agents/capabilities")
            success = response.status_code == 200
            data = response.json()

            if success:
                agents = data.get("data", {}).get("agents", [])
                success = len(agents) >= 5  # è‡³å°‘5ä¸ªæ™ºèƒ½ä½“

                # ä¿å­˜æ™ºèƒ½ä½“èƒ½åŠ›æ•°æ®
                with open(self.test_run_dir / "agents_capabilities.json", 'w', encoding='utf-8') as f:
                    json.dump(data, f, ensure_ascii=False, indent=2)

            self.log_test_result("ç³»ç»Ÿèƒ½åŠ›æŸ¥è¯¢", success, {
                "status_code": response.status_code,
                "agents_count": len(agents) if success else 0
            })
            return success
        except Exception as e:
            self.log_test_result("ç³»ç»Ÿèƒ½åŠ›æŸ¥è¯¢", False, {"error": str(e)})
            return False

    async def test_collaboration_tracking_apis(self) -> bool:
        """æµ‹è¯•åä½œè¿½è¸ªAPIç«¯ç‚¹"""
        try:
            # æµ‹è¯•åä½œä¼šè¯åˆ—è¡¨API
            response = await self.client.get(f"{self.base_url}/api/v1/collaboration/sessions")
            sessions_success = response.status_code == 200
            if sessions_success:
                sessions_data = response.json()
                with open(self.test_run_dir / "collaboration_sessions.json", 'w', encoding='utf-8') as f:
                    json.dump(sessions_data, f, ensure_ascii=False, indent=2)

            # æµ‹è¯•åä½œåˆ†ææ¦‚è§ˆAPI
            response = await self.client.get(f"{self.base_url}/api/v1/collaboration/analytics/overview")
            analytics_success = response.status_code == 200
            if analytics_success:
                analytics_data = response.json()
                with open(self.test_run_dir / "collaboration_analytics.json", 'w', encoding='utf-8') as f:
                    json.dump(analytics_data, f, ensure_ascii=False, indent=2)

            overall_success = sessions_success and analytics_success

            self.log_test_result("åä½œè¿½è¸ªAPI", overall_success, {
                "sessions_api": sessions_success,
                "analytics_api": analytics_success
            })
            return overall_success

        except Exception as e:
            self.log_test_result("åä½œè¿½è¸ªAPI", False, {"error": str(e)})
            return False

    # åˆ é™¤éæ ¸å¿ƒæ¨¡æ¿æµ‹è¯•æ–¹æ³• - ä¸“æ³¨æ ¸å¿ƒå¤šæ™ºèƒ½ä½“åä½œåŠŸèƒ½

    async def test_course_design_session_flow(self) -> bool:
        """æµ‹è¯•å®Œæ•´çš„è¯¾ç¨‹è®¾è®¡ä¼šè¯æµç¨‹"""
        try:
            # 1. åˆ›å»ºè¯¾ç¨‹è®¾è®¡ä¼šè¯
            session_request = {
                "requirements": {
                    "topic": "æœˆçƒæ°¸å±…äººç±»çš„ç§‘æŠ€è£…å¤‡å±•",
                    "audience": "ä¸­å°å­¦ç”Ÿ",
                    "age_group": {"min": 8, "max": 15},
                    "duration": {"days": 3, "hours_per_day": 6},
                    "goals": [
                        "æ¿€å‘å­©å­å¯¹æœªæ¥ç§‘æŠ€å’Œå¤ªç©ºæ¢ç´¢çš„æƒ³è±¡åŠ›",
                        "æŒæ¡3Då»ºæ¨¡å’Œ3Dæ‰“å°çš„åŸºç¡€æŠ€èƒ½",
                        "å­¦ä¼šä½¿ç”¨AIåŠ¨ç”»æŠ€æœ¯åˆ¶ä½œè™šå®èåˆè§†é¢‘",
                        "åŸ¹å…»åˆ›æ–°æ€ç»´å’Œç§‘å­¦æ¢ç´¢ç²¾ç¥",
                        "å®Œæˆæœˆçƒè£…å¤‡çš„å®Œæ•´è®¾è®¡åˆ¶ä½œæµç¨‹"
                    ],
                    "context": "å›½åº†èŠ‚3å¤©AIç§‘æŠ€è®­ç»ƒè¥ï¼Œ6äººå°ç­åˆ¶ï¼Œæœªæ¥ç§‘å¹»ä¸»é¢˜",
                    "constraints": {
                        "budget": "å……è¶³",
                        "equipment": "3Dæ‰“å°æœºã€è®¡ç®—æœºã€AIåŠ¨ç”»è½¯ä»¶ã€æ‘„å½±è®¾å¤‡",
                        "time_limit": "3å¤©å†…å®Œæˆæ‰€æœ‰ä½œå“"
                    },
                    "preferences": {
                        "teaching_style": "é¡¹ç›®åˆ¶å­¦ä¹ +è„‘æ´å¤§å¼€",
                        "assessment_type": "ä½œå“å±•ç¤º+åˆ›æ„è¯„ä»·"
                    },
                    "special_requirements": {
                        "class_size": 6,
                        "final_deliverables": [
                            "æœˆçƒè£…å¤‡è¯´æ˜ä¹¦",
                            "3Dæ‰“å°è£…å¤‡å®ç‰©",
                            "AIåŠ¨ç”»è™šå®èåˆå±•è§ˆè§†é¢‘"
                        ],
                        "theme_focus": "æœˆçƒæ°¸å±…ç”Ÿå­˜è£…å¤‡",
                        "creativity_level": "ç§‘å¹»æœªæ¥å‘"
                    }
                },
                "mode": "full_course",
                "config": {
                    "streaming": True,
                    "max_iterations": 3,
                    "model_preference": "claude",
                    "temperature": 0.7,
                    "enable_collaboration_tracking": True  # é‡è¦ï¼šå¯ç”¨åä½œè¿½è¸ª
                }
            }

            response = await self.client.post(
                f"{self.base_url}/api/v1/agents/sessions",
                json=session_request,
                headers={"Authorization": "Bearer mock_token"}  # æ¨¡æ‹Ÿè®¤è¯
            )

            if response.status_code != 200:
                error_details = {
                    "status_code": response.status_code,
                    "response_text": response.text,
                    "response_headers": dict(response.headers)
                }
                try:
                    error_json = response.json()
                    error_details["response_json"] = error_json
                except:
                    pass
                self.log_test_result("åˆ›å»ºè®¾è®¡ä¼šè¯", False, error_details)
                logger.error(f"åˆ›å»ºè®¾è®¡ä¼šè¯å¤±è´¥ - çŠ¶æ€ç : {response.status_code}, è¯¦ç»†é”™è¯¯: {response.text}")
                return False

            session_data = response.json()["data"]
            session_id = session_data["session_id"]
            self.session_data["session_id"] = session_id

            # 2. å¯åŠ¨è®¾è®¡æµç¨‹ï¼ˆå¼‚æ­¥æ¨¡å¼ï¼‰
            response = await self.client.post(
                f"{self.base_url}/api/v1/agents/sessions/{session_id}/start",
                headers={"Authorization": "Bearer mock_token"}
            )

            if response.status_code not in [200, 202]:  # æ¥å—å¼‚æ­¥å¤„ç†
                error_details = {
                    "status_code": response.status_code,
                    "response_text": response.text,
                    "response_headers": dict(response.headers)
                }
                try:
                    error_json = response.json()
                    error_details["response_json"] = error_json
                except:
                    pass
                self.log_test_result("å¯åŠ¨è®¾è®¡æµç¨‹", False, error_details)
                logger.error(f"å¯åŠ¨è®¾è®¡æµç¨‹å¤±è´¥ - çŠ¶æ€ç : {response.status_code}, è¯¦ç»†é”™è¯¯: {response.text}")
                return False

            logger.info(f"âœ… è®¾è®¡ä»»åŠ¡å·²å¯åŠ¨ï¼Œå¼€å§‹è½®è¯¢çŠ¶æ€...")

            # 3. è½®è¯¢ä¼šè¯çŠ¶æ€ç›´åˆ°å®Œæˆï¼ˆæœ€å¤šç­‰å¾…30åˆ†é’Ÿï¼‰
            max_polls = 180  # 30åˆ†é’Ÿï¼Œæ¯10ç§’è½®è¯¢ä¸€æ¬¡
            poll_interval = 10  # 10ç§’é—´éš”
            status_data = None

            for poll_count in range(max_polls):
                response = await self.client.get(
                    f"{self.base_url}/api/v1/agents/sessions/{session_id}/status",
                    headers={"Authorization": "Bearer mock_token"}
                )

                if response.status_code != 200:
                    self.log_test_result("æŸ¥è¯¢ä¼šè¯çŠ¶æ€", False, {"status_code": response.status_code})
                    return False

                status_data = response.json()["data"]
                status = status_data.get("status", "unknown")
                progress = status_data.get("progress", 0)
                current_agent = status_data.get("current_agent", "unknown")
                current_phase = status_data.get("current_phase", "unknown")
                estimated_remaining = status_data.get("estimated_remaining_seconds")

                logger.info(f"ğŸ“Š è½®è¯¢ {poll_count+1}/{max_polls}: çŠ¶æ€={status}, è¿›åº¦={progress}%, å½“å‰æ™ºèƒ½ä½“={current_agent}, é˜¶æ®µ={current_phase}")

                if estimated_remaining:
                    logger.info(f"â±ï¸ é¢„è®¡å‰©ä½™æ—¶é—´: {estimated_remaining:.0f}ç§’")

                # æ£€æŸ¥æ˜¯å¦å®Œæˆ
                if status == "completed":
                    logger.info(f"ğŸ‰ è®¾è®¡ä»»åŠ¡å®Œæˆï¼æ€»è½®è¯¢æ¬¡æ•°: {poll_count+1}")
                    break
                elif status == "failed":
                    error_msg = status_data.get("error", "æœªçŸ¥é”™è¯¯")
                    logger.error(f"âŒ è®¾è®¡ä»»åŠ¡å¤±è´¥: {error_msg}")

                    # å³ä½¿å¤±è´¥ï¼Œä¹Ÿå°è¯•è·å–éƒ¨åˆ†ç»“æœå’Œåä½œæ•°æ®
                    logger.info("ğŸ”„ å°è¯•è·å–éƒ¨åˆ†ç»“æœå’Œåä½œæ•°æ®...")
                    await self._handle_failed_session(session_id, error_msg)

                    self.log_test_result("è¯¾ç¨‹è®¾è®¡ä¼šè¯æµç¨‹", False, {"error": error_msg})
                    return False
                elif status not in ["running", "created"]:
                    logger.warning(f"âš ï¸ æœªçŸ¥çŠ¶æ€: {status}")

                # å¦‚æœè¿˜åœ¨è¿è¡Œï¼Œç­‰å¾…ä¸‹ä¸€æ¬¡è½®è¯¢
                if poll_count < max_polls - 1:
                    await asyncio.sleep(poll_interval)
            else:
                # è¶…æ—¶äº†
                logger.error(f"âŒ è®¾è®¡ä»»åŠ¡è¶…æ—¶ï¼Œå·²è½®è¯¢ {max_polls} æ¬¡")
                self.log_test_result("è¯¾ç¨‹è®¾è®¡ä¼šè¯æµç¨‹", False, {"error": "ä»»åŠ¡è¶…æ—¶"})
                return False

            # 4. è·å–è®¾è®¡ç»“æœ
            response = await self.client.get(
                f"{self.base_url}/api/v1/agents/sessions/{session_id}/result",
                headers={"Authorization": "Bearer mock_token"}
            )

            if response.status_code != 200:
                self.log_test_result("è·å–è®¾è®¡ç»“æœ", False, {"status_code": response.status_code})
                return False

            result_data = response.json()["data"]
            self.session_data["design_result"] = result_data

            # ä¿å­˜è¯¾ç¨‹è®¾è®¡ç»“æœ
            with open(self.test_run_dir / "course_design_result.json", 'w', encoding='utf-8') as f:
                json.dump(result_data, f, ensure_ascii=False, indent=2)

            # å¯¼å‡ºåä½œè¿½è¸ªæ•°æ®
            await self.export_collaboration_data(session_id)

            # éªŒè¯ç»“æœè´¨é‡
            result_quality_check = self._validate_course_design_result(result_data)

            self.log_test_result("è¯¾ç¨‹è®¾è®¡ä¼šè¯æµç¨‹", True, {
                "session_id": session_id,
                "status": status_data.get("status"),
                "final_progress": status_data.get("progress"),
                "has_result": bool(result_data),
                "result_quality": result_quality_check,
                "total_polls": poll_count + 1
            })
            return True

        except Exception as e:
            self.log_test_result("è¯¾ç¨‹è®¾è®¡ä¼šè¯æµç¨‹", False, {"error": str(e)})
            return False

    async def test_course_iteration_flow(self) -> bool:
        """æµ‹è¯•è¯¾ç¨‹è®¾è®¡è¿­ä»£æµç¨‹"""
        session_id = self.session_data.get("session_id")
        if not session_id:
            self.log_test_result("è¯¾ç¨‹è¿­ä»£æµç¨‹", False, {"error": "ç¼ºå°‘ä¼šè¯ID"})
            return False

        try:
            # æä¾›åé¦ˆè¿›è¡Œè¿­ä»£
            feedback_request = {
                "aspects": {
                    "learning_objectives": "å­¦ä¹ ç›®æ ‡éœ€è¦æ›´åŠ å…·ä½“å’Œå¯è¡¡é‡",
                    "project_design": "é¡¹ç›®ä»»åŠ¡å¯ä»¥æ›´åŠ è´´è¿‘å­¦ç”Ÿå®é™…ç”Ÿæ´»",
                    "assessment": "è¯„ä¼°æ–¹å¼å¯ä»¥å¢åŠ peer reviewç¯èŠ‚"
                },
                "priorities": [
                    "ä¼˜åŒ–é¡¹ç›®ä»»åŠ¡è®¾è®¡",
                    "å®Œå–„è¯„ä¼°æ ‡å‡†"
                ],
                "additional_requirements": {
                    "add_ethics_module": True,
                    "include_industry_cases": True
                }
            }

            response = await self.client.post(
                f"{self.base_url}/api/v1/agents/sessions/{session_id}/iterate",
                json=feedback_request,
                headers={"Authorization": "Bearer mock_token"}
            )

            success = response.status_code == 200

            if success:
                iteration_result = response.json()["data"]
                self.session_data["iteration_result"] = iteration_result

            self.log_test_result("è¯¾ç¨‹è¿­ä»£æµç¨‹", success, {"status_code": response.status_code})
            return success

        except Exception as e:
            self.log_test_result("è¯¾ç¨‹è¿­ä»£æµç¨‹", False, {"error": str(e)})
            return False

    async def test_course_export_flow(self) -> bool:
        """æµ‹è¯•è¯¾ç¨‹å¯¼å‡ºæµç¨‹"""
        session_id = self.session_data.get("session_id")
        if not session_id:
            self.log_test_result("è¯¾ç¨‹å¯¼å‡ºæµç¨‹", False, {"error": "ç¼ºå°‘ä¼šè¯ID"})
            return False

        try:
            # æµ‹è¯•JSONæ ¼å¼å¯¼å‡º
            response = await self.client.get(
                f"{self.base_url}/api/v1/agents/sessions/{session_id}/export?format=json",
                headers={"Authorization": "Bearer mock_token"}
            )

            json_success = response.status_code == 200

            # æµ‹è¯•PDFæ ¼å¼å¯¼å‡º
            response = await self.client.get(
                f"{self.base_url}/api/v1/agents/sessions/{session_id}/export?format=pdf",
                headers={"Authorization": "Bearer mock_token"}
            )

            pdf_success = response.status_code == 200

            # æµ‹è¯•ZIPæ ¼å¼å¯¼å‡º
            response = await self.client.get(
                f"{self.base_url}/api/v1/agents/sessions/{session_id}/export?format=zip",
                headers={"Authorization": "Bearer mock_token"}
            )

            zip_success = response.status_code == 200

            overall_success = json_success or pdf_success or zip_success

            self.log_test_result("è¯¾ç¨‹å¯¼å‡ºæµç¨‹", overall_success, {
                "json_export": json_success,
                "pdf_export": pdf_success,
                "zip_export": zip_success
            })
            return overall_success

        except Exception as e:
            self.log_test_result("è¯¾ç¨‹å¯¼å‡ºæµç¨‹", False, {"error": str(e)})
            return False

    # åˆ é™¤éæ ¸å¿ƒè´¨é‡æ£€æŸ¥æµ‹è¯•æ–¹æ³• - ä¸“æ³¨æ ¸å¿ƒå¤šæ™ºèƒ½ä½“åä½œåŠŸèƒ½

    # åˆ é™¤éæ ¸å¿ƒåä½œåŠŸèƒ½æµ‹è¯•æ–¹æ³• - ä¸“æ³¨æ ¸å¿ƒå¤šæ™ºèƒ½ä½“åä½œåŠŸèƒ½

    async def test_agent_metrics(self) -> bool:
        """æµ‹è¯•æ™ºèƒ½ä½“æŒ‡æ ‡"""
        try:
            response = await self.client.get(
                f"{self.base_url}/api/v1/agents/metrics",
                headers={"Authorization": "Bearer mock_token"}
            )

            success = response.status_code == 200

            if success:
                metrics_data = response.json()["data"]

            self.log_test_result("æ™ºèƒ½ä½“æŒ‡æ ‡", success, {"status_code": response.status_code})
            return success

        except Exception as e:
            self.log_test_result("æ™ºèƒ½ä½“æŒ‡æ ‡", False, {"error": str(e)})
            return False

    async def test_session_cleanup(self) -> bool:
        """æµ‹è¯•ä¼šè¯æ¸…ç†"""
        session_id = self.session_data.get("session_id")
        if not session_id:
            self.log_test_result("ä¼šè¯æ¸…ç†", True, {"message": "æ²¡æœ‰éœ€è¦æ¸…ç†çš„ä¼šè¯"})
            return True

        try:
            response = await self.client.delete(
                f"{self.base_url}/api/v1/agents/sessions/{session_id}",
                headers={"Authorization": "Bearer mock_token"}
            )

            success = response.status_code == 200

            self.log_test_result("ä¼šè¯æ¸…ç†", success, {"status_code": response.status_code})
            return success

        except Exception as e:
            self.log_test_result("ä¼šè¯æ¸…ç†", False, {"error": str(e)})
            return False

    def generate_test_report(self) -> Dict[str, Any]:
        """ç”Ÿæˆæµ‹è¯•æŠ¥å‘Š - å¢å¼ºç‰ˆï¼ŒåŒ…å«åä½œè¿½è¸ªéªŒè¯"""
        total_tests = len(self.test_results)
        successful_tests = sum(1 for result in self.test_results.values() if result["success"])

        # æ£€æŸ¥åä½œè¿½è¸ªç›¸å…³çš„æµ‹è¯•ç»“æœ
        collaboration_tracking_validation = {
            "tracking_api_available": self.test_results.get("åä½œè¿½è¸ªAPI", {}).get("success", False),
            "collaboration_data_exported": self.test_results.get("åä½œæ•°æ®å¯¼å‡º", {}).get("success", False),
            "evidence_found": False,
            "exported_files_count": 0
        }

        # æ£€æŸ¥å¯¼å‡ºçš„åä½œæ•°æ®æ–‡ä»¶
        collaboration_files = [
            "collaboration_flow.json",
            "ai_calls_analytics.json",
            "deliverable_traceability.json",
            "complete_collaboration_report.json"
        ]

        existing_files = []
        for filename in collaboration_files:
            filepath = self.test_run_dir / filename
            if filepath.exists():
                existing_files.append(filename)

        collaboration_tracking_validation["exported_files_count"] = len(existing_files)
        collaboration_tracking_validation["exported_files"] = existing_files

        # æ£€æŸ¥æ˜¯å¦å­˜åœ¨ä¼šè¯å¤±è´¥è®°å½•
        failure_record_path = self.test_run_dir / "session_failure_record.json"
        session_failed = failure_record_path.exists()

        # æ£€æŸ¥è¯¾ç¨‹è®¾è®¡ç»“æœçŠ¶æ€
        design_result = self.session_data.get("design_result", {})
        course_design_successful = False

        if session_failed:
            collaboration_tracking_validation["session_status"] = "failed"
            collaboration_tracking_validation["evidence_found"] = False
            collaboration_tracking_validation["note"] = "ä¼šè¯å¤±è´¥ï¼Œåä½œæ•°æ®ä»…ç”¨äºé—®é¢˜åˆ†æ"
        elif design_result and "collaboration_evidence" in design_result:
            collaboration_tracking_validation["evidence_found"] = True
            collaboration_tracking_validation["session_status"] = "completed"
            course_design_successful = True
        else:
            collaboration_tracking_validation["session_status"] = "unknown"

        report = {
            "test_metadata": {
                "test_start_time": self.test_start_time.isoformat(),
                "test_end_time": datetime.now().isoformat(),
                "export_directory": str(self.test_run_dir)
            },
            "summary": {
                "total_tests": total_tests,
                "successful_tests": successful_tests,
                "failed_tests": total_tests - successful_tests,
                "success_rate": f"{(successful_tests / total_tests * 100):.1f}%" if total_tests > 0 else "0%",
                "test_time": datetime.now().isoformat(),
                "course_design_successful": course_design_successful,
                "quality_assurance": "ä¸¥æ ¼è´¨é‡æ§åˆ¶ - å¤±è´¥æ—¶ä¸æä¾›ä½è´¨é‡å…œåº•æ–¹æ¡ˆ"
            },
            "collaboration_tracking_validation": collaboration_tracking_validation,
            "results": self.test_results,
            "session_data": self.session_data,
            "collaboration_data": self.collaboration_data
        }

        return report

    async def run_complete_business_flow(self) -> Dict[str, Any]:
        """è¿è¡Œå®Œæ•´ä¸šåŠ¡æµç¨‹æµ‹è¯•"""
        logger.info("ğŸš€ å¼€å§‹å®Œæ•´ä¸šåŠ¡æµç¨‹æµ‹è¯•")
        start_time = time.time()

        # ç­‰å¾…æœåŠ¡å¯åŠ¨
        if not await self.wait_and_check_health():
            logger.error("âŒ æœåŠ¡å¥åº·æ£€æŸ¥å¤±è´¥ï¼Œæµ‹è¯•ç»ˆæ­¢")
            return {"error": "æœåŠ¡ä¸å¯ç”¨"}

        # æŒ‰ä¸šåŠ¡æµç¨‹é¡ºåºæ‰§è¡Œæµ‹è¯•
        test_flows = [
            ("åŸºç¡€è¿é€šæ€§æµ‹è¯•", self.test_root_endpoint),
            ("ç³»ç»Ÿèƒ½åŠ›æŸ¥è¯¢", self.test_system_capabilities),
            ("åä½œè¿½è¸ªAPIæµ‹è¯•", self.test_collaboration_tracking_apis),
            ("è¯¾ç¨‹è®¾è®¡ä¼šè¯æµç¨‹", self.test_course_design_session_flow),
            ("è¯¾ç¨‹è¿­ä»£ä¼˜åŒ–æµç¨‹", self.test_course_iteration_flow),
            ("è¯¾ç¨‹å¯¼å‡ºåŠŸèƒ½", self.test_course_export_flow),
            ("æ™ºèƒ½ä½“æ€§èƒ½æŒ‡æ ‡", self.test_agent_metrics),
            ("ä¼šè¯æ¸…ç†", self.test_session_cleanup),
        ]

        # æ‰§è¡Œæ‰€æœ‰æµ‹è¯•
        for test_name, test_func in test_flows:
            logger.info(f"ğŸ“‹ æ‰§è¡Œæµ‹è¯•: {test_name}")
            try:
                await test_func()
            except Exception as e:
                logger.error(f"æµ‹è¯•å¼‚å¸¸: {test_name} - {str(e)}")
                self.log_test_result(test_name, False, {"error": str(e)})

            # æµ‹è¯•é—´éš”
            await asyncio.sleep(0.5)

        # ç”Ÿæˆæµ‹è¯•æŠ¥å‘Š
        report = self.generate_test_report()
        execution_time = time.time() - start_time
        report["execution_time_seconds"] = round(execution_time, 2)

        # è¾“å‡ºæµ‹è¯•ç»“æœ
        logger.info(f"ğŸ‰ æµ‹è¯•å®Œæˆï¼Œè€—æ—¶: {execution_time:.2f}ç§’")
        logger.info(f"ğŸ“Š æˆåŠŸç‡: {report['summary']['success_rate']}")

        return report


async def main():
    """ä¸»å‡½æ•°"""
    # ä»ç¯å¢ƒå˜é‡è·å–é…ç½®
    base_url = os.getenv("TEST_BASE_URL", "http://localhost:48284")

    # åˆ›å»ºæµ‹è¯•å™¨
    tester = BusinessFlowTester(base_url)

    try:
        # è¿è¡Œæµ‹è¯•
        report = await tester.run_complete_business_flow()

        # ä¿å­˜æµ‹è¯•æŠ¥å‘Šåˆ°exportsç›®å½•
        if hasattr(tester, 'test_run_dir'):
            report_path = tester.test_run_dir / "business_flow_report.json"
        else:
            report_path = Path(__file__).parent / "test_reports" / f"business_flow_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
            report_path.parent.mkdir(parents=True, exist_ok=True)

        with open(report_path, 'w', encoding='utf-8') as f:
            json.dump(report, f, indent=2, ensure_ascii=False)

        logger.info(f"ğŸ“„ æµ‹è¯•æŠ¥å‘Šå·²ä¿å­˜: {report_path}")

        # ç”Ÿæˆå¯è¯»çš„æ‘˜è¦æ–‡æ¡£
        if hasattr(tester, 'test_run_dir'):
            summary_path = tester.test_run_dir / "business_flow_summary.md"
            collaboration_validation = report.get("collaboration_tracking_validation", {})

            summary_text = f"""# ä¸šåŠ¡æµç¨‹æµ‹è¯•æŠ¥å‘Š - åä½œè¿½è¸ªéªŒè¯

## æµ‹è¯•æ¦‚è§ˆ
- æµ‹è¯•æ—¶é—´: {report.get('test_metadata', {}).get('test_start_time', 'N/A')}
- å¯¼å‡ºç›®å½•: {report.get('test_metadata', {}).get('export_directory', 'N/A')}
- æ€»æµ‹è¯•æ•°: {report['summary']['total_tests']}
- æˆåŠŸæµ‹è¯•: {report['summary']['successful_tests']}
- å¤±è´¥æµ‹è¯•: {report['summary']['failed_tests']}
- æˆåŠŸç‡: {report['summary']['success_rate']}

## åä½œè¿½è¸ªéªŒè¯ç»“æœ
- è¿½è¸ªAPIå¯ç”¨: {'âœ…' if collaboration_validation.get('tracking_api_available') else 'âŒ'}
- åä½œæ•°æ®å¯¼å‡º: {'âœ…' if collaboration_validation.get('collaboration_data_exported') else 'âŒ'}
- åä½œè¯æ®å‘ç°: {'âœ…' if collaboration_validation.get('evidence_found') else 'âŒ'}
- å¯¼å‡ºæ–‡ä»¶æ•°é‡: {collaboration_validation.get('exported_files_count', 0)}

## å¯¼å‡ºçš„åä½œè¿½è¸ªæ–‡ä»¶
{chr(10).join(f'- {filename}' for filename in collaboration_validation.get('exported_files', []))}

## æµ‹è¯•ç»“è®º
{'ğŸ‰ åä½œè¿½è¸ªåŠŸèƒ½éªŒè¯æˆåŠŸï¼å®Œæ•´çš„JSONæŠ¥å‘Šå·²ä¿å­˜ï¼Œå¯ä»¥é€šè¿‡å¯¼å‡ºæ–‡ä»¶éªŒè¯å¤šæ™ºèƒ½ä½“åä½œè¿‡ç¨‹ã€‚' if collaboration_validation.get('collaboration_data_exported') else 'âš ï¸ åä½œè¿½è¸ªåŠŸèƒ½éªŒè¯ä¸å®Œæ•´ï¼Œè¯·æ£€æŸ¥è¯¦ç»†æµ‹è¯•ç»“æœã€‚'}
"""

            with open(summary_path, 'w', encoding='utf-8') as f:
                f.write(summary_text)

            logger.info(f"ğŸ“„ æµ‹è¯•æ‘˜è¦å·²ä¿å­˜: {summary_path}")

        # è¾“å‡ºå…³é”®ç»“æœ
        if "summary" in report:
            summary = report["summary"]
            print("\n" + "="*60)
            print("ğŸ¯ ä¸šåŠ¡æµç¨‹æµ‹è¯•ç»“æœæ±‡æ€»")
            print("="*60)
            print(f"æ€»æµ‹è¯•æ•°: {summary['total_tests']}")
            print(f"æˆåŠŸæµ‹è¯•: {summary['successful_tests']}")
            print(f"å¤±è´¥æµ‹è¯•: {summary['failed_tests']}")
            print(f"æˆåŠŸç‡: {summary['success_rate']}")
            print(f"æ‰§è¡Œæ—¶é—´: {report.get('execution_time_seconds', 0)}ç§’")
            print("="*60)

            # è¾“å‡ºåä½œè¿½è¸ªéªŒè¯ç»“æœ
            if "collaboration_tracking_validation" in report:
                collaboration_validation = report["collaboration_tracking_validation"]
                print("\nğŸ¯ åä½œè¿½è¸ªåŠŸèƒ½éªŒè¯ç»“æœ")
                print("="*40)
                print(f"è¿½è¸ªAPIå¯ç”¨: {'âœ…' if collaboration_validation.get('tracking_api_available') else 'âŒ'}")
                print(f"åä½œæ•°æ®å¯¼å‡º: {'âœ…' if collaboration_validation.get('collaboration_data_exported') else 'âŒ'}")
                print(f"åä½œè¯æ®å‘ç°: {'âœ…' if collaboration_validation.get('evidence_found') else 'âŒ'}")
                print(f"å¯¼å‡ºæ–‡ä»¶æ•°é‡: {collaboration_validation.get('exported_files_count', 0)}")

                if collaboration_validation.get('exported_files'):
                    print("å¯¼å‡ºçš„åä½œæ–‡ä»¶:")
                    for filename in collaboration_validation.get('exported_files', []):
                        print(f"  - {filename}")

                if hasattr(tester, 'test_run_dir'):
                    print(f"ğŸ“ å®Œæ•´æŠ¥å‘Šä½ç½®: {tester.test_run_dir}")

                print("="*40)

            # å¤±è´¥çš„æµ‹è¯•è¯¦æƒ…
            failed_tests = [name for name, result in report["results"].items() if not result["success"]]
            if failed_tests:
                print("âŒ å¤±è´¥çš„æµ‹è¯•:")
                for test_name in failed_tests:
                    details = report["results"][test_name].get("details", {})
                    error = details.get("error", "æœªçŸ¥é”™è¯¯")
                    print(f"  - {test_name}: {error}")
            else:
                print("âœ… æ‰€æœ‰æµ‹è¯•é€šè¿‡!")
        else:
            print("âŒ æµ‹è¯•å¼‚å¸¸ç»“æŸ")
            print(json.dumps(report, indent=2, ensure_ascii=False))

        # æ ¸å¿ƒä¸šåŠ¡æµç¨‹æµ‹è¯•å¿…é¡»100%é€šè¿‡
        success_rate_str = report.get("summary", {}).get("success_rate", "0%")
        success_rate = float(success_rate_str.rstrip('%'))
        failed_tests = report.get("summary", {}).get("failed_tests", 1)

        if success_rate == 100.0 and failed_tests == 0:
            print(f"ğŸ¯ æˆåŠŸç‡ {success_rate_str}ï¼Œæ‰€æœ‰æ ¸å¿ƒä¸šåŠ¡æµç¨‹æµ‹è¯•é€šè¿‡")
            return 0
        else:
            print(f"âŒ æˆåŠŸç‡ {success_rate_str}ï¼Œæ ¸å¿ƒä¸šåŠ¡æµç¨‹æµ‹è¯•æœªå®Œå…¨é€šè¿‡")
            print("æ ¸å¿ƒä¸šåŠ¡æ¥å£å¿…é¡»100%æˆåŠŸæ‰èƒ½é€šè¿‡æµ‹è¯•")
            return 1

    except KeyboardInterrupt:
        logger.info("ç”¨æˆ·ä¸­æ–­æµ‹è¯•")
        return 130
    except Exception as e:
        logger.error(f"æµ‹è¯•æ‰§è¡Œå¼‚å¸¸: {str(e)}")
        return 1
    finally:
        await tester.cleanup()


if __name__ == "__main__":
    exit_code = asyncio.run(main())
    sys.exit(exit_code)