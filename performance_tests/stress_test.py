"""
å‹åŠ›æµ‹è¯•å’Œæ€§èƒ½åŸºå‡†æµ‹è¯•
"""
import asyncio
import aiohttp
import time
import statistics
import json
import logging
from concurrent.futures import ThreadPoolExecutor, as_completed
from dataclasses import dataclass
from typing import List, Dict, Any, Optional
import psutil
import matplotlib.pyplot as plt
import pandas as pd
from datetime import datetime, timedelta

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)


@dataclass
class PerformanceMetrics:
    """æ€§èƒ½æŒ‡æ ‡æ•°æ®ç±»"""
    response_times: List[float]
    success_count: int
    error_count: int
    throughput: float  # requests per second
    cpu_usage: List[float]
    memory_usage: List[float]
    timestamp: datetime
    
    @property
    def avg_response_time(self) -> float:
        return statistics.mean(self.response_times) if self.response_times else 0
    
    @property
    def p95_response_time(self) -> float:
        return statistics.quantiles(self.response_times, n=20)[18] if len(self.response_times) > 20 else 0
    
    @property
    def p99_response_time(self) -> float:
        return statistics.quantiles(self.response_times, n=100)[98] if len(self.response_times) > 100 else 0
    
    @property
    def error_rate(self) -> float:
        total = self.success_count + self.error_count
        return self.error_count / total if total > 0 else 0
    
    @property
    def avg_cpu_usage(self) -> float:
        return statistics.mean(self.cpu_usage) if self.cpu_usage else 0
    
    @property
    def avg_memory_usage(self) -> float:
        return statistics.mean(self.memory_usage) if self.memory_usage else 0


class SystemMonitor:
    """ç³»ç»Ÿèµ„æºç›‘æ§å™¨"""
    
    def __init__(self):
        self.monitoring = False
        self.metrics = []
        
    async def start_monitoring(self, interval: float = 1.0):
        """å¼€å§‹ç›‘æ§ç³»ç»Ÿèµ„æº"""
        self.monitoring = True
        while self.monitoring:
            cpu_percent = psutil.cpu_percent()
            memory_info = psutil.virtual_memory()
            
            self.metrics.append({
                'timestamp': datetime.now(),
                'cpu_percent': cpu_percent,
                'memory_percent': memory_info.percent,
                'memory_available': memory_info.available / 1024 / 1024,  # MB
            })
            
            await asyncio.sleep(interval)
    
    def stop_monitoring(self):
        """åœæ­¢ç›‘æ§"""
        self.monitoring = False
    
    def get_metrics(self) -> List[Dict]:
        """è·å–ç›‘æ§æŒ‡æ ‡"""
        return self.metrics.copy()


class PerformanceTester:
    """æ€§èƒ½æµ‹è¯•å™¨"""
    
    def __init__(self, base_url: str):
        self.base_url = base_url
        self.session = None
        self.monitor = SystemMonitor()
        
    async def __aenter__(self):
        self.session = aiohttp.ClientSession(
            timeout=aiohttp.ClientTimeout(total=30),
            connector=aiohttp.TCPConnector(limit=100)
        )
        return self
    
    async def __aexit__(self, exc_type, exc_val, exc_tb):
        if self.session:
            await self.session.close()
    
    async def single_request(self, endpoint: str, method: str = 'GET', 
                           data: Optional[Dict] = None, headers: Optional[Dict] = None) -> Dict[str, Any]:
        """å•ä¸ªè¯·æ±‚æµ‹è¯•"""
        url = f"{self.base_url}{endpoint}"
        start_time = time.time()
        
        try:
            async with self.session.request(method, url, json=data, headers=headers) as response:
                response_time = (time.time() - start_time) * 1000  # ms
                content = await response.text()
                
                return {
                    'success': response.status < 400,
                    'status_code': response.status,
                    'response_time': response_time,
                    'content_length': len(content),
                    'error': None
                }
        except Exception as e:
            response_time = (time.time() - start_time) * 1000
            return {
                'success': False,
                'status_code': 0,
                'response_time': response_time,
                'content_length': 0,
                'error': str(e)
            }
    
    async def concurrent_requests(self, endpoint: str, num_requests: int, 
                                concurrent_users: int, method: str = 'GET',
                                data: Optional[Dict] = None) -> PerformanceMetrics:
        """å¹¶å‘è¯·æ±‚æµ‹è¯•"""
        logger.info(f"å¼€å§‹å¹¶å‘æµ‹è¯•: {num_requests} è¯·æ±‚, {concurrent_users} å¹¶å‘ç”¨æˆ·")
        
        # å¼€å§‹ç³»ç»Ÿç›‘æ§
        monitor_task = asyncio.create_task(self.monitor.start_monitoring())
        
        semaphore = asyncio.Semaphore(concurrent_users)
        
        async def make_request():
            async with semaphore:
                return await self.single_request(endpoint, method, data)
        
        start_time = time.time()
        
        # æ‰§è¡Œå¹¶å‘è¯·æ±‚
        tasks = [make_request() for _ in range(num_requests)]
        results = await asyncio.gather(*tasks)
        
        end_time = time.time()
        total_time = end_time - start_time
        
        # åœæ­¢ç›‘æ§
        self.monitor.stop_monitoring()
        monitor_task.cancel()
        
        # åˆ†æç»“æœ
        response_times = [r['response_time'] for r in results]
        success_count = sum(1 for r in results if r['success'])
        error_count = len(results) - success_count
        throughput = num_requests / total_time
        
        # è·å–ç³»ç»Ÿç›‘æ§æ•°æ®
        system_metrics = self.monitor.get_metrics()
        cpu_usage = [m['cpu_percent'] for m in system_metrics]
        memory_usage = [m['memory_percent'] for m in system_metrics]
        
        return PerformanceMetrics(
            response_times=response_times,
            success_count=success_count,
            error_count=error_count,
            throughput=throughput,
            cpu_usage=cpu_usage,
            memory_usage=memory_usage,
            timestamp=datetime.now()
        )
    
    async def stress_test(self, endpoint: str, max_users: int = 1000, 
                         step_size: int = 50, step_duration: int = 60) -> List[PerformanceMetrics]:
        """å‹åŠ›æµ‹è¯• - é€æ­¥å¢åŠ å¹¶å‘ç”¨æˆ·æ•°"""
        logger.info(f"å¼€å§‹å‹åŠ›æµ‹è¯•: æœ€å¤§ {max_users} ç”¨æˆ·, æ­¥é•¿ {step_size}")
        
        results = []
        
        for users in range(step_size, max_users + 1, step_size):
            logger.info(f"æµ‹è¯• {users} å¹¶å‘ç”¨æˆ·...")
            
            metrics = await self.concurrent_requests(
                endpoint, 
                num_requests=users * 2,  # æ¯ä¸ªç”¨æˆ·2ä¸ªè¯·æ±‚
                concurrent_users=users
            )
            
            results.append(metrics)
            
            # æ£€æŸ¥æ˜¯å¦è¾¾åˆ°æ€§èƒ½é˜ˆå€¼
            if metrics.error_rate > 0.1 or metrics.avg_response_time > 10000:
                logger.warning(f"æ€§èƒ½é˜ˆå€¼å·²è¾¾åˆ°ï¼Œåœæ­¢å¢åŠ è´Ÿè½½")
                break
            
            # ç­‰å¾…ç³»ç»Ÿæ¢å¤
            await asyncio.sleep(5)
        
        return results
    
    async def endurance_test(self, endpoint: str, duration_minutes: int = 30,
                           concurrent_users: int = 100) -> List[PerformanceMetrics]:
        """è€ä¹…æ€§æµ‹è¯• - é•¿æ—¶é—´ç¨³å®šè´Ÿè½½"""
        logger.info(f"å¼€å§‹è€ä¹…æ€§æµ‹è¯•: {duration_minutes} åˆ†é’Ÿ, {concurrent_users} å¹¶å‘ç”¨æˆ·")
        
        results = []
        end_time = time.time() + (duration_minutes * 60)
        
        while time.time() < end_time:
            metrics = await self.concurrent_requests(
                endpoint,
                num_requests=concurrent_users * 5,  # æ¯è½®æ¯ç”¨æˆ·5ä¸ªè¯·æ±‚
                concurrent_users=concurrent_users
            )
            
            results.append(metrics)
            
            # è®°å½•å½“å‰çŠ¶æ€
            logger.info(f"è€ä¹…æµ‹è¯•è¿›è¡Œä¸­ - å¹³å‡å“åº”æ—¶é—´: {metrics.avg_response_time:.2f}ms, "
                       f"é”™è¯¯ç‡: {metrics.error_rate:.2%}, CPU: {metrics.avg_cpu_usage:.1f}%")
            
            # çŸ­æš‚ä¼‘æ¯
            await asyncio.sleep(10)
        
        return results


class SpecializedTests:
    """ä¸“é—¨çš„æ€§èƒ½æµ‹è¯•"""
    
    def __init__(self, base_url: str):
        self.base_url = base_url
    
    async def agent_collaboration_stress_test(self) -> Dict[str, Any]:
        """æ™ºèƒ½ä½“åä½œå‹åŠ›æµ‹è¯•"""
        logger.info("å¼€å§‹æ™ºèƒ½ä½“åä½œå‹åŠ›æµ‹è¯•")
        
        async with PerformanceTester(self.base_url) as tester:
            # å‡†å¤‡æµ‹è¯•æ•°æ®
            collaboration_data = {
                "task": "design_complete_course",
                "course_requirements": {
                    "title": "å‹åŠ›æµ‹è¯•è¯¾ç¨‹",
                    "subject": "è®¡ç®—æœºç§‘å­¦",
                    "grade_level": "é«˜ä¸­",
                    "duration_weeks": 12
                },
                "agents": ["education_director", "learning_designer", "creative_technologist"]
            }
            
            # æµ‹è¯•ä¸åŒå¹¶å‘çº§åˆ«çš„æ™ºèƒ½ä½“åä½œ
            results = {}
            for concurrent_collaborations in [1, 5, 10, 20]:
                logger.info(f"æµ‹è¯• {concurrent_collaborations} ä¸ªå¹¶å‘åä½œ")
                
                metrics = await tester.concurrent_requests(
                    "/api/v1/agents/collaborate",
                    num_requests=concurrent_collaborations,
                    concurrent_users=concurrent_collaborations,
                    method='POST',
                    data=collaboration_data
                )
                
                results[f"{concurrent_collaborations}_concurrent"] = {
                    'avg_response_time': metrics.avg_response_time,
                    'p95_response_time': metrics.p95_response_time,
                    'error_rate': metrics.error_rate,
                    'throughput': metrics.throughput
                }
            
            return results
    
    async def websocket_connection_test(self) -> Dict[str, Any]:
        """WebSocketè¿æ¥å‹åŠ›æµ‹è¯•"""
        logger.info("å¼€å§‹WebSocketè¿æ¥å‹åŠ›æµ‹è¯•")
        
        # æ¨¡æ‹ŸWebSocketè¿æ¥æµ‹è¯•
        # å®é™…å®ç°éœ€è¦ä½¿ç”¨websocketsåº“
        
        connection_results = []
        max_connections = 1000
        
        for num_connections in range(100, max_connections + 1, 100):
            start_time = time.time()
            
            # æ¨¡æ‹Ÿè¿æ¥å»ºç«‹
            successful_connections = 0
            failed_connections = 0
            
            try:
                # è¿™é‡Œåº”è¯¥å®ç°å®é™…çš„WebSocketè¿æ¥é€»è¾‘
                # ä¸ºäº†æ¼”ç¤ºï¼Œæˆ‘ä»¬æ¨¡æ‹Ÿè¿æ¥è¿‡ç¨‹
                await asyncio.sleep(0.1 * num_connections / 100)  # æ¨¡æ‹Ÿè¿æ¥æ—¶é—´
                successful_connections = num_connections
            except Exception:
                failed_connections = num_connections
            
            connection_time = time.time() - start_time
            
            connection_results.append({
                'num_connections': num_connections,
                'successful_connections': successful_connections,
                'failed_connections': failed_connections,
                'connection_time': connection_time,
                'success_rate': successful_connections / num_connections
            })
            
            # å¦‚æœæˆåŠŸç‡ä½äº80%ï¼Œåœæ­¢æµ‹è¯•
            if successful_connections / num_connections < 0.8:
                logger.warning("WebSocketè¿æ¥æˆåŠŸç‡è¿‡ä½ï¼Œåœæ­¢æµ‹è¯•")
                break
        
        return {'connection_results': connection_results}
    
    async def database_performance_test(self) -> Dict[str, Any]:
        """æ•°æ®åº“æ€§èƒ½æµ‹è¯•"""
        logger.info("å¼€å§‹æ•°æ®åº“æ€§èƒ½æµ‹è¯•")
        
        async with PerformanceTester(self.base_url) as tester:
            results = {}
            
            # æµ‹è¯•è¯»æ“ä½œæ€§èƒ½
            read_metrics = await tester.concurrent_requests(
                "/api/v1/courses/",
                num_requests=1000,
                concurrent_users=50
            )
            
            results['read_performance'] = {
                'avg_response_time': read_metrics.avg_response_time,
                'throughput': read_metrics.throughput,
                'error_rate': read_metrics.error_rate
            }
            
            # æµ‹è¯•å†™æ“ä½œæ€§èƒ½
            course_data = {
                "title": "æ€§èƒ½æµ‹è¯•è¯¾ç¨‹",
                "description": "æ•°æ®åº“æ€§èƒ½æµ‹è¯•",
                "subject": "æµ‹è¯•",
                "grade_level": "æµ‹è¯•",
                "duration_weeks": 1
            }
            
            write_metrics = await tester.concurrent_requests(
                "/api/v1/courses/",
                num_requests=200,
                concurrent_users=20,
                method='POST',
                data=course_data
            )
            
            results['write_performance'] = {
                'avg_response_time': write_metrics.avg_response_time,
                'throughput': write_metrics.throughput,
                'error_rate': write_metrics.error_rate
            }
            
            return results


class PerformanceReporter:
    """æ€§èƒ½æµ‹è¯•æŠ¥å‘Šç”Ÿæˆå™¨"""
    
    @staticmethod
    def generate_report(test_results: Dict[str, Any], output_file: str = "performance_report.html"):
        """ç”Ÿæˆæ€§èƒ½æµ‹è¯•æŠ¥å‘Š"""
        logger.info(f"ç”Ÿæˆæ€§èƒ½æµ‹è¯•æŠ¥å‘Š: {output_file}")
        
        html_content = f"""
        <!DOCTYPE html>
        <html>
        <head>
            <title>PBLæ™ºèƒ½åŠ©æ‰‹ç³»ç»Ÿæ€§èƒ½æµ‹è¯•æŠ¥å‘Š</title>
            <style>
                body {{ font-family: Arial, sans-serif; margin: 20px; }}
                .metric {{ background: #f5f5f5; padding: 10px; margin: 10px 0; border-radius: 5px; }}
                .success {{ color: green; }}
                .warning {{ color: orange; }}
                .error {{ color: red; }}
                table {{ border-collapse: collapse; width: 100%; }}
                th, td {{ border: 1px solid #ddd; padding: 8px; text-align: left; }}
                th {{ background-color: #f2f2f2; }}
            </style>
        </head>
        <body>
            <h1>PBLæ™ºèƒ½åŠ©æ‰‹ç³»ç»Ÿæ€§èƒ½æµ‹è¯•æŠ¥å‘Š</h1>
            <p>æµ‹è¯•æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}</p>
            
            <h2>æµ‹è¯•æ‘˜è¦</h2>
            <div class="metric">
                <h3>æ•´ä½“æ€§èƒ½æŒ‡æ ‡</h3>
                {PerformanceReporter._format_summary(test_results)}
            </div>
            
            <h2>è¯¦ç»†æµ‹è¯•ç»“æœ</h2>
            {PerformanceReporter._format_detailed_results(test_results)}
            
            <h2>æ€§èƒ½å»ºè®®</h2>
            {PerformanceReporter._generate_recommendations(test_results)}
        </body>
        </html>
        """
        
        with open(output_file, 'w', encoding='utf-8') as f:
            f.write(html_content)
    
    @staticmethod
    def _format_summary(results: Dict[str, Any]) -> str:
        """æ ¼å¼åŒ–æ‘˜è¦ä¿¡æ¯"""
        summary_html = "<ul>"
        
        for test_name, test_data in results.items():
            if isinstance(test_data, dict) and 'avg_response_time' in test_data:
                response_time = test_data['avg_response_time']
                error_rate = test_data.get('error_rate', 0)
                
                status_class = "success" if response_time < 2000 and error_rate < 0.05 else "warning"
                if response_time > 5000 or error_rate > 0.1:
                    status_class = "error"
                
                summary_html += f"""
                <li class="{status_class}">
                    <strong>{test_name}</strong>: 
                    å¹³å‡å“åº”æ—¶é—´ {response_time:.2f}ms, 
                    é”™è¯¯ç‡ {error_rate:.2%}
                </li>
                """
        
        summary_html += "</ul>"
        return summary_html
    
    @staticmethod
    def _format_detailed_results(results: Dict[str, Any]) -> str:
        """æ ¼å¼åŒ–è¯¦ç»†ç»“æœ"""
        detailed_html = ""
        
        for test_name, test_data in results.items():
            detailed_html += f"<h3>{test_name}</h3>"
            
            if isinstance(test_data, dict):
                detailed_html += "<table>"
                detailed_html += "<tr><th>æŒ‡æ ‡</th><th>å€¼</th></tr>"
                
                for key, value in test_data.items():
                    if isinstance(value, float):
                        if 'time' in key.lower():
                            formatted_value = f"{value:.2f} ms"
                        elif 'rate' in key.lower():
                            formatted_value = f"{value:.2%}"
                        else:
                            formatted_value = f"{value:.2f}"
                    else:
                        formatted_value = str(value)
                    
                    detailed_html += f"<tr><td>{key}</td><td>{formatted_value}</td></tr>"
                
                detailed_html += "</table>"
        
        return detailed_html
    
    @staticmethod
    def _generate_recommendations(results: Dict[str, Any]) -> str:
        """ç”Ÿæˆæ€§èƒ½ä¼˜åŒ–å»ºè®®"""
        recommendations = []
        
        # åˆ†æç»“æœå¹¶ç”Ÿæˆå»ºè®®
        for test_name, test_data in results.items():
            if isinstance(test_data, dict) and 'avg_response_time' in test_data:
                response_time = test_data['avg_response_time']
                error_rate = test_data.get('error_rate', 0)
                
                if response_time > 5000:
                    recommendations.append(f"âš ï¸ {test_name} å“åº”æ—¶é—´è¿‡é•¿ï¼Œå»ºè®®ä¼˜åŒ–æ•°æ®åº“æŸ¥è¯¢æˆ–å¢åŠ ç¼“å­˜")
                
                if error_rate > 0.1:
                    recommendations.append(f"âŒ {test_name} é”™è¯¯ç‡è¿‡é«˜ï¼Œéœ€è¦æ£€æŸ¥ç³»ç»Ÿç¨³å®šæ€§")
                
                if response_time > 2000:
                    recommendations.append(f"ğŸ”§ {test_name} å¯ä»¥è€ƒè™‘å®æ–½ä»¥ä¸‹ä¼˜åŒ–ï¼šå¼‚æ­¥å¤„ç†ã€è¿æ¥æ± ä¼˜åŒ–ã€CDNéƒ¨ç½²")
        
        if not recommendations:
            recommendations.append("âœ… ç³»ç»Ÿæ€§èƒ½è¡¨ç°è‰¯å¥½ï¼Œç»§ç»­ä¿æŒç›‘æ§")
        
        recommendations_html = "<ul>"
        for rec in recommendations:
            recommendations_html += f"<li>{rec}</li>"
        recommendations_html += "</ul>"
        
        return recommendations_html


async def run_complete_performance_test(base_url: str = "http://localhost:8000"):
    """è¿è¡Œå®Œæ•´çš„æ€§èƒ½æµ‹è¯•å¥—ä»¶"""
    logger.info("å¼€å§‹å®Œæ•´æ€§èƒ½æµ‹è¯•å¥—ä»¶")
    
    all_results = {}
    
    # 1. åŸºç¡€APIæ€§èƒ½æµ‹è¯•
    logger.info("=== åŸºç¡€APIæ€§èƒ½æµ‹è¯• ===")
    async with PerformanceTester(base_url) as tester:
        # è¯¾ç¨‹åˆ—è¡¨API
        courses_metrics = await tester.concurrent_requests(
            "/api/v1/courses/", 
            num_requests=500, 
            concurrent_users=50
        )
        all_results['courses_api'] = {
            'avg_response_time': courses_metrics.avg_response_time,
            'p95_response_time': courses_metrics.p95_response_time,
            'error_rate': courses_metrics.error_rate,
            'throughput': courses_metrics.throughput
        }
        
        # ç”¨æˆ·è®¤è¯API
        auth_data = {"username": "testuser", "password": "testpass"}
        auth_metrics = await tester.concurrent_requests(
            "/api/v1/auth/login",
            num_requests=200,
            concurrent_users=20,
            method='POST',
            data=auth_data
        )
        all_results['auth_api'] = {
            'avg_response_time': auth_metrics.avg_response_time,
            'p95_response_time': auth_metrics.p95_response_time,
            'error_rate': auth_metrics.error_rate,
            'throughput': auth_metrics.throughput
        }
    
    # 2. ä¸“é—¨æµ‹è¯•
    logger.info("=== ä¸“é—¨æ€§èƒ½æµ‹è¯• ===")
    specialized = SpecializedTests(base_url)
    
    agent_results = await specialized.agent_collaboration_stress_test()
    all_results['agent_collaboration'] = agent_results
    
    websocket_results = await specialized.websocket_connection_test()
    all_results['websocket_performance'] = websocket_results
    
    db_results = await specialized.database_performance_test()
    all_results['database_performance'] = db_results
    
    # 3. å‹åŠ›æµ‹è¯•
    logger.info("=== å‹åŠ›æµ‹è¯• ===")
    async with PerformanceTester(base_url) as tester:
        stress_results = await tester.stress_test("/api/v1/courses/", max_users=200, step_size=25)
        
        # æ‰¾åˆ°æœ€é«˜æ€§èƒ½ç‚¹
        best_performance = min(stress_results, key=lambda x: x.avg_response_time)
        all_results['stress_test'] = {
            'max_stable_users': len(stress_results) * 25,
            'best_avg_response_time': best_performance.avg_response_time,
            'best_throughput': best_performance.throughput,
            'peak_cpu_usage': max(best_performance.cpu_usage) if best_performance.cpu_usage else 0
        }
    
    # 4. ç”ŸæˆæŠ¥å‘Š
    logger.info("=== ç”Ÿæˆæ€§èƒ½æµ‹è¯•æŠ¥å‘Š ===")
    PerformanceReporter.generate_report(all_results)
    
    # è¾“å‡ºå…³é”®æŒ‡æ ‡
    logger.info("=== æµ‹è¯•å®Œæˆ - å…³é”®æŒ‡æ ‡ ===")
    for test_name, results in all_results.items():
        if isinstance(results, dict) and 'avg_response_time' in results:
            logger.info(f"{test_name}: {results['avg_response_time']:.2f}ms å¹³å‡å“åº”æ—¶é—´")
    
    return all_results


if __name__ == "__main__":
    # è¿è¡Œå®Œæ•´æ€§èƒ½æµ‹è¯•
    asyncio.run(run_complete_performance_test())